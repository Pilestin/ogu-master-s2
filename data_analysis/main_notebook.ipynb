{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbee0d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Imbalanced-learn imports\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Boosting imports\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization, Dropout\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "# Optuna for hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c77c1d",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fde567",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"Veri setini yükler.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Dosya bulunamadı: {filepath}\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Veri ön işleme adımlarını gerçekleştirir.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # 1. 'Revenue' ve 'Weekend' boolean sütunlarını sayısal değerlere dönüştürün\n",
    "    df_processed['Revenue'] = df_processed['Revenue'].astype(int)\n",
    "    df_processed['Weekend'] = df_processed['Weekend'].astype(int)\n",
    "\n",
    "    # 2. 'Month' ve 'VisitorType' gibi kategorik sütunları one-hot encoding kullanarak sayısal temsilcilere dönüştürün\n",
    "    df_processed = pd.get_dummies(df_processed, columns=['Month', 'VisitorType'], drop_first=True)\n",
    "\n",
    "    print(\"Veri ön işleme tamamlandı.\")\n",
    "    return df_processed\n",
    "\n",
    "def perform_hypothesis_testing(df):\n",
    "    \"\"\"\n",
    "    Veri seti üzerinde hipotez testleri uygular.\n",
    "    Numerik değişkenler için T-Testi, Kategorik değişkenler için Chi-Square testi.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Hipotez Testleri Başlatılıyor ---\")\n",
    "    \n",
    "    target = 'Revenue'\n",
    "    if target not in df.columns:\n",
    "        print(\"Hedef değişken bulunamadı.\")\n",
    "        return\n",
    "\n",
    "    # Grupları ayır\n",
    "    group_true = df[df[target] == 1]\n",
    "    group_false = df[df[target] == 0]\n",
    "\n",
    "    print(f\"Grup True (Revenue=1) Sayısı: {len(group_true)}\")\n",
    "    print(f\"Grup False (Revenue=0) Sayısı: {len(group_false)}\")\n",
    "\n",
    "    # Sütun tiplerini belirle\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    # Revenue ve one-hot encoded olmayan kategorik sütunları çıkar (eğer varsa)\n",
    "    numeric_cols = [c for c in numeric_cols if c != target and 'Month' not in c and 'VisitorType' not in c] \n",
    "    \n",
    "    # One-hot encoded sütunları kategorik olarak kabul edebiliriz ama orijinal halleri yoksa\n",
    "    # binary oldukları için Chi-Square uygundur.\n",
    "    # Ancak burada df_processed geldiği için one-hot sütunlar var.\n",
    "    # Biz sadece temel numeriklere bakalım: Administrative, Administrative_Duration, etc.\n",
    "    \n",
    "    basic_numeric_cols = [\n",
    "        'Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration',\n",
    "        'ProductRelated', 'ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n1. Numerik Değişkenler için T-Testi (Bağımsız Örneklem):\")\n",
    "    print(f\"{'Değişken':<30} | {'T-Statistic':<12} | {'P-Value':<12} | {'Sonuç'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for col in basic_numeric_cols:\n",
    "        if col in df.columns:\n",
    "            stat, p_value = stats.ttest_ind(group_true[col], group_false[col], equal_var=False)\n",
    "            significance = \"Anlamlı Fark Var (H0 Red)\" if p_value < 0.05 else \"Fark Yok (H0 Kabul)\"\n",
    "            print(f\"{col:<30} | {stat:.4f}       | {p_value:.4e}   | {significance}\")\n",
    "\n",
    "    print(\"\\n2. Kategorik/Binary Değişkenler için Chi-Square Testi:\")\n",
    "    # Weekend ve One-Hot encoded sütunlar\n",
    "    categorical_cols = ['Weekend'] + [c for c in df.columns if 'Month_' in c or 'VisitorType_' in c]\n",
    "    \n",
    "    print(f\"{'Değişken':<30} | {'Chi2 Stat':<12} | {'P-Value':<12} | {'Sonuç'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            contingency_table = pd.crosstab(df[col], df[target])\n",
    "            stat, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "            significance = \"Bağımlı (H0 Red)\" if p < 0.05 else \"Bağımsız (H0 Kabul)\"\n",
    "            print(f\"{col:<30} | {stat:.4f}       | {p:.4e}   | {significance}\")\n",
    "            \n",
    "    print(\"--- Hipotez Testleri Tamamlandı ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb94936",
   "metadata": {},
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ac49c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_comparison_report(results_df, scenario_best_models, output_dir, best_params=None):\n",
    "    \"\"\"Karşılaştırma raporu ve görselleri oluşturur.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SONUÇ RAPORU\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Her Senaryo için en iyi sonuçlar (TÜM METRİKLER)\n",
    "    print(\"\\n--- Her Senaryo İçin En İyi Model (Tüm Metrikler) ---\")\n",
    "    scenario_summary = []\n",
    "    for scenario, metrics in scenario_best_models.items():\n",
    "        print(f\"\\n{scenario}:\")\n",
    "        print(f\"   Model: {metrics['Model']}\")\n",
    "        print(f\"   Accuracy:  {metrics['Accuracy']:.4f}\")\n",
    "        print(f\"   Precision: {metrics['Precision']:.4f}\")\n",
    "        print(f\"   Recall:    {metrics['Recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {metrics['F1-Score']:.4f}\")\n",
    "        print(f\"   ROC AUC:   {metrics['ROC AUC']:.4f}\")\n",
    "        scenario_summary.append({\n",
    "            'Senaryo': scenario,\n",
    "            'En İyi Model': metrics['Model'],\n",
    "            'Accuracy': metrics['Accuracy'],\n",
    "            'Precision': metrics['Precision'],\n",
    "            'Recall': metrics['Recall'],\n",
    "            'F1-Score': metrics['F1-Score'],\n",
    "            'ROC AUC': metrics['ROC AUC']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(scenario_summary)\n",
    "    \n",
    "    # 2. Tekniklerin Katkısı (birden fazla metrik için)\n",
    "    print(\"\\n--- TEKNİKLERİN KATKISI (Accuracy ve F1 bazında) ---\")\n",
    "    baseline_f1 = scenario_best_models['Baseline']['F1-Score']\n",
    "    baseline_acc = scenario_best_models['Baseline']['Accuracy']\n",
    "    \n",
    "    contributions = {}\n",
    "    for scenario, metrics in scenario_best_models.items():\n",
    "        if scenario != 'Baseline':\n",
    "            f1_improvement = ((metrics['F1-Score'] - baseline_f1) / baseline_f1) * 100\n",
    "            acc_improvement = ((metrics['Accuracy'] - baseline_acc) / baseline_acc) * 100\n",
    "            contributions[scenario] = {'F1': f1_improvement, 'Accuracy': acc_improvement}\n",
    "            print(f\"{scenario}:\")\n",
    "            print(f\"   F1-Score:  {f1_improvement:+.2f}%\")\n",
    "            print(f\"   Accuracy:  {acc_improvement:+.2f}%\")\n",
    "    \n",
    "    # 3. Sonuçları kaydet\n",
    "    results_df.to_csv(os.path.join(output_dir, \"comparison_results.csv\"), index=False)\n",
    "    summary_df.to_csv(os.path.join(output_dir, \"scenario_summary.csv\"), index=False)\n",
    "    print(f\"\\nSonuçlar kaydedildi: {output_dir}\")\n",
    "    \n",
    "    # 4. Görselleştirmeler\n",
    "    create_visualizations(summary_df, contributions, output_dir, results_df)\n",
    "    \n",
    "    # 5. Optuna Parametre Tablosu Görseli\n",
    "    if best_params:\n",
    "        create_parameter_table(best_params, output_dir)\n",
    "    \n",
    "    return summary_df, contributions\n",
    "\n",
    "\n",
    "\n",
    "def create_parameter_table(best_params, output_dir):\n",
    "    \"\"\"Optuna ile bulunan en iyi parametrelerin tablo görselini oluşturur.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Parametre verilerini hazırla\n",
    "    table_data = []\n",
    "    for model_name, params in best_params.items():\n",
    "        for param, value in params.items():\n",
    "            if isinstance(value, float):\n",
    "                table_data.append([model_name.upper(), param, f\"{value:.4f}\"])\n",
    "            else:\n",
    "                table_data.append([model_name.upper(), param, str(value)])\n",
    "    \n",
    "    # Tablo oluştur\n",
    "    table = ax.table(\n",
    "        cellText=table_data,\n",
    "        colLabels=['Model', 'Parametre', 'Değer'],\n",
    "        cellLoc='left',\n",
    "        loc='center',\n",
    "        colWidths=[0.25, 0.35, 0.4]\n",
    "    )\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.8)\n",
    "    \n",
    "    # Başlık renklerini ayarla\n",
    "    for i in range(3):\n",
    "        table[(0, i)].set_facecolor('#3498db')\n",
    "        table[(0, i)].set_text_props(color='white', fontweight='bold')\n",
    "    \n",
    "    # Satır renklerini model bazında ayarla\n",
    "    colors = {'XGBOOST': '#e8f4fd', 'LIGHTGBM': '#e8fdf4', 'RANDOM_FOREST': '#fdf4e8'}\n",
    "    for i, row in enumerate(table_data):\n",
    "        model = row[0]\n",
    "        bg_color = colors.get(model, '#ffffff')\n",
    "        for j in range(3):\n",
    "            table[(i+1, j)].set_facecolor(bg_color)\n",
    "    \n",
    "    ax.set_title('Optuna ile Bulunan En İyi Hiperparametreler', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"optuna_parameters.png\"), dpi=150, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Parametre tablosu kaydedildi: {os.path.join(output_dir, 'optuna_parameters.png')}\")\n",
    "\n",
    "\n",
    "def create_visualizations(summary_df, contributions, output_dir, results_df=None):\n",
    "    \"\"\"Karşılaştırma grafiklerini oluşturur.\"\"\"\n",
    "    \n",
    "    # Stil ayarları\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\n",
    "    \n",
    "    # 1. F1-Score Karşılaştırması ve Teknik Katkıları\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Sol: F1 ve ROC-AUC bar chart\n",
    "    x = np.arange(len(summary_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0].bar(x - width/2, summary_df['F1-Score'], width, label='F1-Score', color=colors[0])\n",
    "    bars2 = axes[0].bar(x + width/2, summary_df['ROC AUC'], width, label='ROC AUC', color=colors[1])\n",
    "    \n",
    "    axes[0].set_xlabel('Senaryo', fontsize=11)\n",
    "    axes[0].set_ylabel('Skor', fontsize=11)\n",
    "    axes[0].set_title('Senaryo Bazlı Performans Karşılaştırması', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(summary_df['Senaryo'], rotation=15, ha='right')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    for bar in bars1:\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    for bar in bars2:\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Sağ: Teknik Katkıları (F1 ve Accuracy)\n",
    "    if contributions:\n",
    "        scenarios = list(contributions.keys())\n",
    "        f1_values = [contributions[s]['F1'] for s in scenarios]\n",
    "        acc_values = [contributions[s]['Accuracy'] for s in scenarios]\n",
    "        \n",
    "        y_pos = np.arange(len(scenarios))\n",
    "        height = 0.35\n",
    "        \n",
    "        bars1 = axes[1].barh(y_pos - height/2, f1_values, height, label='F1-Score', \n",
    "                            color=[colors[1] if v >= 0 else colors[3] for v in f1_values])\n",
    "        bars2 = axes[1].barh(y_pos + height/2, acc_values, height, label='Accuracy', \n",
    "                            color=[colors[0] if v >= 0 else colors[3] for v in acc_values], alpha=0.7)\n",
    "        \n",
    "        axes[1].set_yticks(y_pos)\n",
    "        axes[1].set_yticklabels(scenarios)\n",
    "        axes[1].axvline(x=0, color='gray', linestyle='--', linewidth=1)\n",
    "        axes[1].set_xlabel('% İyileşme (Baseline\\'a Göre)', fontsize=11)\n",
    "        axes[1].set_title('Her Tekniğin Katkısı (F1 ve Accuracy)', fontsize=13, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        for bar, val in zip(bars1, f1_values):\n",
    "            axes[1].text(val + 0.3 if val >= 0 else val - 0.3, bar.get_y() + bar.get_height()/2,\n",
    "                        f'{val:+.1f}%', ha='left' if val >= 0 else 'right', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"comparison_barplot.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Tüm metriklerin heatmap'i\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    metrics_cols = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']\n",
    "    heatmap_data = summary_df[metrics_cols].values\n",
    "    \n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                xticklabels=metrics_cols, yticklabels=summary_df['Senaryo'],\n",
    "                ax=ax, vmin=0.3, vmax=1.0, cbar_kws={'label': 'Skor'})\n",
    "    \n",
    "    ax.set_title('Tüm Metrikler - Senaryo Karşılaştırması', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"metrics_heatmap.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Her Metrik için Senaryo Bazlı Değişim Grafikleri (5 ayrı grafik)\n",
    "    if results_df is not None:\n",
    "        create_detailed_metric_plots(results_df, output_dir)\n",
    "    \n",
    "    print(f\"\\nGörseller kaydedildi: {output_dir}\")\n",
    "\n",
    "\n",
    "def create_detailed_metric_plots(results_df, output_dir):\n",
    "    \"\"\"Her metrik için senaryo bazlı detaylı grafikler oluşturur.\"\"\"\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']\n",
    "    scenarios = results_df['Senaryo'].unique()\n",
    "    models = results_df[results_df['Senaryo'] == scenarios[0]]['Model'].unique()\n",
    "    \n",
    "    # Renk paleti\n",
    "    n_models = len(models)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, n_models))\n",
    "    \n",
    "    # 3.1 Tüm metriklerin senaryolara göre değişimi (Line Plot)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            model_data = results_df[results_df['Model'] == model]\n",
    "            if len(model_data) > 0:\n",
    "                scenario_order = ['1. Baseline', '2. SMOTE Only', '3. Feature Selection Only', '4. SMOTE + FS + Optuna']\n",
    "                model_data_sorted = model_data.set_index('Senaryo').reindex([s for s in scenario_order if s in model_data['Senaryo'].values])\n",
    "                \n",
    "                ax.plot(range(len(model_data_sorted)), model_data_sorted[metric].values, \n",
    "                       marker='o', label=model, color=colors[i], linewidth=2, markersize=6)\n",
    "        \n",
    "        ax.set_xlabel('Senaryo', fontsize=10)\n",
    "        ax.set_ylabel(metric, fontsize=10)\n",
    "        ax.set_title(f'{metric} - Senaryo Değişimi', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(range(len(scenario_order)))\n",
    "        ax.set_xticklabels(['Baseline', 'SMOTE', 'FS', 'Full'], fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Son subplot'u legend için kullan\n",
    "    axes[5].axis('off')\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    axes[5].legend(handles, labels, loc='center', fontsize=9, ncol=2, title='Modeller')\n",
    "    \n",
    "    plt.suptitle('Tüm Metriklerin Senaryolara Göre Değişimi', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"metrics_by_scenario_lines.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3.2 Her senaryo için model karşılaştırma bar chart\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    scenario_short = {\n",
    "        '1. Baseline': 'Baseline',\n",
    "        '2. SMOTE Only': 'SMOTE Only',\n",
    "        '3. Feature Selection Only': 'Feature Selection',\n",
    "        '4. SMOTE + FS + Optuna': 'Full Pipeline'\n",
    "    }\n",
    "    \n",
    "    for idx, scenario in enumerate(scenarios[:4]):\n",
    "        ax = axes[idx]\n",
    "        scenario_data = results_df[results_df['Senaryo'] == scenario].copy()\n",
    "        scenario_data = scenario_data.sort_values('F1-Score', ascending=True)\n",
    "        \n",
    "        y_pos = np.arange(len(scenario_data))\n",
    "        \n",
    "        bars = ax.barh(y_pos, scenario_data['F1-Score'], color=plt.cm.viridis(scenario_data['F1-Score']))\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(scenario_data['Model'], fontsize=9)\n",
    "        ax.set_xlabel('F1-Score', fontsize=10)\n",
    "        ax.set_title(f'{scenario_short.get(scenario, scenario)}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlim(0, 1)\n",
    "        \n",
    "        for bar, val in zip(bars, scenario_data['F1-Score']):\n",
    "            ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{val:.3f}', va='center', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Her Senaryo İçin Model F1-Score Karşılaştırması', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"models_by_scenario_bars.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3.3 Tüm sonuçlar için büyük heatmap\n",
    "    pivot_f1 = results_df.pivot_table(values='F1-Score', index='Model', columns='Senaryo', aggfunc='first')\n",
    "    \n",
    "    # Sütun sırasını düzenle\n",
    "    col_order = ['1. Baseline', '2. SMOTE Only', '3. Feature Selection Only', '4. SMOTE + FS + Optuna']\n",
    "    pivot_f1 = pivot_f1[[c for c in col_order if c in pivot_f1.columns]]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                ax=ax, vmin=0.3, vmax=0.8, cbar_kws={'label': 'F1-Score'},\n",
    "                linewidths=0.5)\n",
    "    \n",
    "    ax.set_title('Tüm Modeller - F1-Score Karşılaştırması', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Senaryo', fontsize=11)\n",
    "    ax.set_ylabel('Model', fontsize=11)\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"all_models_heatmap.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b565a4c",
   "metadata": {},
   "source": [
    "### TRAIN-EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e883ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_ml_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Klasik makine öğrenimi modellerini eğitir ve değerlendirir.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(random_state=42, solver='liblinear'),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        \"LightGBM\": LGBMClassifier(random_state=42, verbose=-1),\n",
    "        \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0, iterations=100),\n",
    "        \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "    }\n",
    "\n",
    "    # SVM Özel durumu (olasılık ve parametreler)\n",
    "    print(\"\\nEğitiliyor: SVM (RBF kernel)...\")\n",
    "    svm_model = SVC(kernel=\"rbf\", C=10, gamma=0.1, class_weight=\"balanced\", probability=True, random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    y_pred_proba_svm = svm_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_svm = (y_pred_proba_svm >= 0.3).astype(int) # Threshold 0.3\n",
    "    results[\"SVM (RBF kernel)\"] = evaluate_model(\"SVM (RBF kernel)\", y_test, y_pred_svm, y_pred_proba_svm)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEğitiliyor: {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        y_pred_proba = None\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "        results[name] = evaluate_model(name, y_test, y_pred, y_pred_proba)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def train_dl_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Derin öğrenme modellerini (Keras MLP ve TabNet) eğitir ve değerlendirir.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Veriyi ölçeklendirme (DL modelleri için önemli)\n",
    "    scaler_dl = StandardScaler()\n",
    "    X_train_scaled = scaler_dl.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_dl.transform(X_test)\n",
    "    \n",
    "    # --- Keras MLP Model ---\n",
    "    print(\"\\nEğitiliyor: Enhanced Deep Learning (Keras)...\")\n",
    "    model_enhanced = Sequential([\n",
    "        InputLayer(input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(8, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model_enhanced.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model_enhanced.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=25,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        verbose=0 # Çıktıyı temiz tutmak için 0\n",
    "    )\n",
    "    \n",
    "    y_pred_proba_dl = model_enhanced.predict(X_test_scaled, verbose=0)\n",
    "    y_pred_dl = (y_pred_proba_dl > 0.5).astype(int).flatten()\n",
    "    y_pred_proba_dl = y_pred_proba_dl.flatten()\n",
    "    \n",
    "    results[\"Enhanced Deep Learning\"] = evaluate_model(\"Enhanced Deep Learning\", y_test, y_pred_dl, y_pred_proba_dl)\n",
    "    \n",
    "    # --- TabNet Model ---\n",
    "    print(\"\\nEğitiliyor: TabNet...\")\n",
    "    tabnet_model = TabNetClassifier(\n",
    "        n_d=16, n_a=16, n_steps=5, gamma=1.5,\n",
    "        n_independent=2, n_shared=2,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        mask_type='entmax',\n",
    "        seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    tabnet_model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train.values,\n",
    "        eval_set=[(X_test_scaled, y_test.values)],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=200,\n",
    "        patience=20,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    y_pred_tabnet = tabnet_model.predict(X_test_scaled)\n",
    "    y_pred_proba_tabnet = tabnet_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    results[\"TabNet\"] = evaluate_model(\"TabNet\", y_test, y_pred_tabnet, y_pred_proba_tabnet)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7eebc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(name, y_test, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Model performansını değerlendirir ve sonuçları döndürür.\"\"\"\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    roc_auc = 0\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        except ValueError:\n",
    "            roc_auc = 0\n",
    "\n",
    "    print(f\"\\n----- {name} Performans Metrikleri -----\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC AUC\": roc_auc\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_dl_models(X_train, y_train, X_test, y_test, scenario_name):\n",
    "    \"\"\"Deep Learning modelleri (Keras MLP ve TabNet) için değerlendirme yapar.\"\"\"\n",
    "    dl_results = []\n",
    "    \n",
    "    # Veri tipini numpy array olarak ayarla (her birini ayrı kontrol et)\n",
    "    if hasattr(X_train, 'values'):\n",
    "        X_train_np = X_train.values\n",
    "    else:\n",
    "        X_train_np = np.array(X_train)\n",
    "    \n",
    "    if hasattr(X_test, 'values'):\n",
    "        X_test_np = X_test.values\n",
    "    else:\n",
    "        X_test_np = np.array(X_test)\n",
    "    \n",
    "    if hasattr(y_train, 'values'):\n",
    "        y_train_np = y_train.values\n",
    "    else:\n",
    "        y_train_np = np.array(y_train)\n",
    "    \n",
    "    if hasattr(y_test, 'values'):\n",
    "        y_test_np = y_test.values\n",
    "    else:\n",
    "        y_test_np = np.array(y_test)\n",
    "    \n",
    "    # --- Keras Enhanced Deep Learning Model ---\n",
    "    print(\"    Eğitiliyor: Enhanced Deep Learning...\")\n",
    "    model_keras = Sequential([\n",
    "        InputLayer(input_shape=(X_train_np.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(8, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model_keras.fit(\n",
    "        X_train_np, y_train_np,\n",
    "        epochs=25,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_test_np, y_test_np),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    y_pred_proba_keras = model_keras.predict(X_test_np, verbose=0).flatten()\n",
    "    y_pred_keras = (y_pred_proba_keras > 0.5).astype(int)\n",
    "    \n",
    "    dl_results.append({\n",
    "        \"Model\": \"Enhanced Deep Learning\",\n",
    "        \"Senaryo\": scenario_name,\n",
    "        \"Accuracy\": accuracy_score(y_test_np, y_pred_keras),\n",
    "        \"Precision\": precision_score(y_test_np, y_pred_keras, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test_np, y_pred_keras),\n",
    "        \"F1-Score\": f1_score(y_test_np, y_pred_keras),\n",
    "        \"ROC AUC\": roc_auc_score(y_test_np, y_pred_proba_keras)\n",
    "    })\n",
    "    print(f\"    Enhanced Deep Learning: F1={dl_results[-1]['F1-Score']:.4f}, ROC-AUC={dl_results[-1]['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # --- TabNet Model ---\n",
    "    print(\"    Eğitiliyor: TabNet...\")\n",
    "    tabnet_model = TabNetClassifier(\n",
    "        n_d=16, n_a=16, n_steps=5, gamma=1.5,\n",
    "        n_independent=2, n_shared=2,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        mask_type='entmax',\n",
    "        seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    tabnet_model.fit(\n",
    "        X_train_np,\n",
    "        y_train_np,\n",
    "        eval_set=[(X_test_np, y_test_np)],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    y_pred_tabnet = tabnet_model.predict(X_test_np)\n",
    "    y_pred_proba_tabnet = tabnet_model.predict_proba(X_test_np)[:, 1]\n",
    "    \n",
    "    dl_results.append({\n",
    "        \"Model\": \"TabNet\",\n",
    "        \"Senaryo\": scenario_name,\n",
    "        \"Accuracy\": accuracy_score(y_test_np, y_pred_tabnet),\n",
    "        \"Precision\": precision_score(y_test_np, y_pred_tabnet, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test_np, y_pred_tabnet),\n",
    "        \"F1-Score\": f1_score(y_test_np, y_pred_tabnet),\n",
    "        \"ROC AUC\": roc_auc_score(y_test_np, y_pred_proba_tabnet)\n",
    "    })\n",
    "    print(f\"    TabNet: F1={dl_results[-1]['F1-Score']:.4f}, ROC-AUC={dl_results[-1]['ROC AUC']:.4f}\")\n",
    "    \n",
    "    return dl_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124b546",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_comparative_analysis(X_train_orig, y_train_orig, X_test_orig, y_test, \n",
    "                              feature_names, output_dir):\n",
    "    \"\"\"\n",
    "    4 farklı senaryo ile karşılaştırmalı analiz yapar:\n",
    "    1. Baseline (ham veri)\n",
    "    2. Sadece SMOTE\n",
    "    3. Sadece Feature Selection\n",
    "    4. SMOTE + Feature Selection + Optuna\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KARŞILAŞTIRMALI ANALİZ BAŞLIYOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    scenario_best_models = {}\n",
    "    \n",
    "    # Kullanılacak temel modeller (11 model)\n",
    "    def get_base_models():\n",
    "        return {\n",
    "            \"Logistic Regression\": LogisticRegression(random_state=42, solver='liblinear', max_iter=1000),\n",
    "            \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "            \"Random Forest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            \"SVM (RBF kernel)\": SVC(kernel=\"rbf\", C=10, gamma=0.1, class_weight=\"balanced\", probability=True, random_state=42),\n",
    "            \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "            \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "            \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "            \"LightGBM\": LGBMClassifier(random_state=42, verbose=-1),\n",
    "            \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0, iterations=100)\n",
    "        }\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SENARYO 1: BASELINE (Ham Veri)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"SENARYO 1: BASELINE (Ham Veri, Varsayılan Parametreler)\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_orig)\n",
    "    X_test_scaled = scaler.transform(X_test_orig)\n",
    "    \n",
    "    scenario1_results = []\n",
    "    for name, model in get_base_models().items():\n",
    "        metrics = evaluate_scenario(X_train_scaled, y_train_orig, X_test_scaled, y_test, model, name)\n",
    "        metrics[\"Senaryo\"] = \"1. Baseline\"\n",
    "        scenario1_results.append(metrics)\n",
    "        print(f\"  {name}: F1={metrics['F1-Score']:.4f}, ROC-AUC={metrics['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Deep Learning modelleri\n",
    "    dl_s1 = evaluate_dl_models(X_train_scaled, y_train_orig, X_test_scaled, y_test, \"1. Baseline\")\n",
    "    scenario1_results.extend(dl_s1)\n",
    "    \n",
    "    results.extend(scenario1_results)\n",
    "    best_s1 = max(scenario1_results, key=lambda x: x['F1-Score'])\n",
    "    scenario_best_models['Baseline'] = best_s1\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SENARYO 2: SADECE SMOTE\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"SENARYO 2: SADECE SMOTE\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    X_train_smote, y_train_smote = apply_smote(\n",
    "        pd.DataFrame(X_train_scaled, columns=feature_names), \n",
    "        y_train_orig, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    scenario2_results = []\n",
    "    for name, model in get_base_models().items():\n",
    "        metrics = evaluate_scenario(X_train_smote, y_train_smote, X_test_scaled, y_test, model, name)\n",
    "        metrics[\"Senaryo\"] = \"2. SMOTE Only\"\n",
    "        scenario2_results.append(metrics)\n",
    "        print(f\"  {name}: F1={metrics['F1-Score']:.4f}, ROC-AUC={metrics['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Deep Learning modelleri\n",
    "    dl_s2 = evaluate_dl_models(X_train_smote, y_train_smote, X_test_scaled, y_test, \"2. SMOTE Only\")\n",
    "    scenario2_results.extend(dl_s2)\n",
    "    \n",
    "    results.extend(scenario2_results)\n",
    "    best_s2 = max(scenario2_results, key=lambda x: x['F1-Score'])\n",
    "    scenario_best_models['SMOTE Only'] = best_s2\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SENARYO 3: SADECE FEATURE SELECTION\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"SENARYO 3: SADECE FEATURE SELECTION\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Feature selection uygula\n",
    "    X_train_fs, X_test_fs = perform_feature_selection(\n",
    "        pd.DataFrame(X_train_scaled, columns=feature_names, index=X_train_orig.index),\n",
    "        y_train_orig,\n",
    "        pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test_orig.index)\n",
    "    )\n",
    "    \n",
    "    scenario3_results = []\n",
    "    for name, model in get_base_models().items():\n",
    "        metrics = evaluate_scenario(X_train_fs, y_train_orig, X_test_fs, y_test, model, name)\n",
    "        metrics[\"Senaryo\"] = \"3. Feature Selection Only\"\n",
    "        scenario3_results.append(metrics)\n",
    "        print(f\"  {name}: F1={metrics['F1-Score']:.4f}, ROC-AUC={metrics['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Deep Learning modelleri\n",
    "    dl_s3 = evaluate_dl_models(X_train_fs, y_train_orig, X_test_fs, y_test, \"3. Feature Selection Only\")\n",
    "    scenario3_results.extend(dl_s3)\n",
    "    \n",
    "    results.extend(scenario3_results)\n",
    "    best_s3 = max(scenario3_results, key=lambda x: x['F1-Score'])\n",
    "    scenario_best_models['Feature Selection Only'] = best_s3\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SENARYO 4: SMOTE + FEATURE SELECTION + OPTUNA\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"SENARYO 4: SMOTE + FEATURE SELECTION + OPTUNA\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Önce SMOTE uygula, sonra Feature Selection\n",
    "    X_train_smote_fs, y_train_smote_fs = apply_smote(X_train_fs, y_train_orig, verbose=False)\n",
    "    \n",
    "    scenario4_results = []\n",
    "    \n",
    "    # Önce tüm temel modelleri varsayılan parametrelerle dene (adil karşılaştırma için)\n",
    "    print(\"\\n  [Temel Modeller - Varsayılan Parametreler]\")\n",
    "    for name, model in get_base_models().items():\n",
    "        metrics = evaluate_scenario(X_train_smote_fs, y_train_smote_fs, X_test_fs, y_test, model, name)\n",
    "        metrics[\"Senaryo\"] = \"4. SMOTE + FS + Optuna\"\n",
    "        scenario4_results.append(metrics)\n",
    "        print(f\"  {name}: F1={metrics['F1-Score']:.4f}, ROC-AUC={metrics['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Optuna ile hiperparametre optimizasyonu\n",
    "    best_params = run_optuna_optimization(X_train_smote_fs, y_train_smote_fs, n_trials=25)\n",
    "    \n",
    "    # Optuna ile optimize edilmiş modeller (ek olarak)\n",
    "    print(\"\\n  [Optuna Optimize Modeller]\")\n",
    "    optimized_models = {\n",
    "        \"XGBoost (Optuna)\": XGBClassifier(\n",
    "            **best_params['xgboost'], \n",
    "            random_state=42, \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='logloss'\n",
    "        ),\n",
    "        \"LightGBM (Optuna)\": LGBMClassifier(\n",
    "            **best_params['lightgbm'], \n",
    "            random_state=42, \n",
    "            verbose=-1\n",
    "        ),\n",
    "        \"Random Forest (Optuna)\": RandomForestClassifier(\n",
    "            **best_params['random_forest'], \n",
    "            random_state=42, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    for name, model in optimized_models.items():\n",
    "        metrics = evaluate_scenario(X_train_smote_fs, y_train_smote_fs, X_test_fs, y_test, model, name)\n",
    "        metrics[\"Senaryo\"] = \"4. SMOTE + FS + Optuna\"\n",
    "        scenario4_results.append(metrics)\n",
    "        print(f\"  {name}: F1={metrics['F1-Score']:.4f}, ROC-AUC={metrics['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Not: Senaryo 4'te Deep Learning modelleri çalıştırılmıyor (hız optimizasyonu)\n",
    "    # Temel ML modelleri + Optuna optimize modeller yeterli karşılaştırma sağlıyor\n",
    "    \n",
    "    results.extend(scenario4_results)\n",
    "    best_s4 = max(scenario4_results, key=lambda x: x['F1-Score'])\n",
    "    scenario_best_models['Full Pipeline'] = best_s4\n",
    "    \n",
    "    return pd.DataFrame(results), scenario_best_models, best_params\n",
    "\n",
    "def perform_clustering(df_processed):\n",
    "    \"\"\"K-Means kümeleme işlemini gerçekleştirir.\"\"\"\n",
    "    print(\"Kümeleme işlemi başlatılıyor...\")\n",
    "    \n",
    "    # 1. Hedef değişken 'Revenue' hariç tüm özelliklerden oluşan bir DataFrame oluşturun\n",
    "    X_cluster = df_processed.drop('Revenue', axis=1)\n",
    "\n",
    "    # 2. X_cluster verisini standartlaştırın\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "    # 3. Seçilen optimal küme sayısını (örneğin 4) kullanarak KMeans modelini eğitin\n",
    "    optimal_k = 4 \n",
    "    kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    df_clustered = df_processed.copy()\n",
    "    df_clustered['Cluster'] = clusters\n",
    "\n",
    "    print(f\"K-Means clustering completed with {optimal_k} clusters.\")\n",
    "    print(\"Cluster distribution:\")\n",
    "    print(df_clustered['Cluster'].value_counts())\n",
    "    \n",
    "    return df_clustered\n",
    "\n",
    "def get_data_splits(df_processed):\n",
    "    \"\"\"Veriyi eğitim ve test setlerine ayırır.\"\"\"\n",
    "    # Not: Notebook'ta kümeleme yapıldıktan sonra 'Cluster' sütunu eğitim verisinden çıkarılıyor.\n",
    "    # Eğer df_processed içinde 'Cluster' varsa çıkaralım.\n",
    "    \n",
    "    drop_cols = ['Revenue']\n",
    "    if 'Cluster' in df_processed.columns:\n",
    "        drop_cols.append('Cluster')\n",
    "        \n",
    "    X = df_processed.drop(drop_cols, axis=1)\n",
    "    y = df_processed['Revenue']\n",
    "\n",
    "    # Stratifiye edilmiş bölme\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"Veri eğitim ve test setlerine ayrıldı.\")\n",
    "    print(f\"X_train boyutu: {X_train.shape}, y_train boyutu: {y_train.shape}\")\n",
    "    print(f\"X_test boyutu: {X_test.shape}, y_test boyutu: {y_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268174a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KARŞILAŞTIRMALI ANALİZ\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_scenario(X_train, y_train, X_test, y_test, model, model_name):\n",
    "    \"\"\"Tek bir model için değerlendirme yapar.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_pred_proba = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred),\n",
    "        \"ROC AUC\": roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else 0\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d0a7c",
   "metadata": {},
   "source": [
    "### FS - Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805c617",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def perform_feature_selection(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Random Forest kullanarak özellik seçimi yapar.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Özellik Seçimi (Feature Selection) Başlatılıyor ---\")\n",
    "    print(f\"Orijinal Özellik Sayısı: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Özellik seçimi için temel bir model kullan\n",
    "    selector_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    selector_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Özellik önemlerini al\n",
    "    importances = selector_model.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Önem derecelerini göster\n",
    "    feature_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "    print(\"\\nEn Önemli 10 Özellik:\")\n",
    "    print(feature_imp_df.head(10))\n",
    "    \n",
    "    # SelectFromModel ile seçim yap (mean değerinden yüksek olanları seç)\n",
    "    selector = SelectFromModel(selector_model, threshold='mean', prefit=True)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    selected_features = feature_names[selector.get_support()]\n",
    "    print(f\"\\nSeçilen Özellik Sayısı: {X_train_selected.shape[1]}\")\n",
    "    print(f\"Seçilen Özellikler: {list(selected_features)}\")\n",
    "    \n",
    "    # DataFrame formatını korumak için (sütun isimleri kaybolmasın diye)\n",
    "    X_train_selected_df = pd.DataFrame(X_train_selected, columns=selected_features, index=X_train.index)\n",
    "    X_test_selected_df = pd.DataFrame(X_test_selected, columns=selected_features, index=X_test.index)\n",
    "    \n",
    "    print(\"--- Özellik Seçimi Tamamlandı ---\\n\")\n",
    "    return X_train_selected_df, X_test_selected_df\n",
    "\n",
    "def apply_smote(X_train, y_train, verbose=True):\n",
    "    \"\"\"\n",
    "    SMOTE (Synthetic Minority Over-sampling Technique) uygulayarak veri dengesizliğini giderir.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n--- SMOTE ile Veri Dengeleme Başlatılıyor ---\")\n",
    "        print(\"Önceki Sınıf Dağılımı:\")\n",
    "        print(y_train.value_counts())\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nSonraki Sınıf Dağılımı:\")\n",
    "        print(y_train_resampled.value_counts())\n",
    "        print(\"--- SMOTE Tamamlandı ---\\n\")\n",
    "    \n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OPTUNA HİPERPARAMETRE OPTİMİZASYONU\n",
    "# =============================================================================\n",
    "\n",
    "def optimize_xgboost_optuna(X_train, y_train, n_trials=30):\n",
    "    \"\"\"XGBoost için Optuna ile hiperparametre optimizasyonu.\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'random_state': 42,\n",
    "            'use_label_encoder': False,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "\n",
    "def optimize_lightgbm_optuna(X_train, y_train, n_trials=30):\n",
    "    \"\"\"LightGBM için Optuna ile hiperparametre optimizasyonu.\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        model = LGBMClassifier(**params)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "\n",
    "def optimize_random_forest_optuna(X_train, y_train, n_trials=30):\n",
    "    \"\"\"Random Forest için Optuna ile hiperparametre optimizasyonu.\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = RandomForestClassifier(**params)\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "\n",
    "def run_optuna_optimization(X_train, y_train, n_trials=30):\n",
    "    \"\"\"Tüm modeller için Optuna optimizasyonu yapar.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTUNA HİPERPARAMETRE OPTİMİZASYONU BAŞLIYOR\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_params = {}\n",
    "    \n",
    "    print(\"\\n[1/3] XGBoost optimizasyonu...\")\n",
    "    xgb_params, xgb_score = optimize_xgboost_optuna(X_train, y_train, n_trials)\n",
    "    best_params['xgboost'] = xgb_params\n",
    "    print(f\"   En iyi CV F1 Skoru: {xgb_score:.4f}\")\n",
    "    \n",
    "    print(\"\\n[2/3] LightGBM optimizasyonu...\")\n",
    "    lgbm_params, lgbm_score = optimize_lightgbm_optuna(X_train, y_train, n_trials)\n",
    "    best_params['lightgbm'] = lgbm_params\n",
    "    print(f\"   En iyi CV F1 Skoru: {lgbm_score:.4f}\")\n",
    "    \n",
    "    print(\"\\n[3/3] Random Forest optimizasyonu...\")\n",
    "    rf_params, rf_score = optimize_random_forest_optuna(X_train, y_train, n_trials)\n",
    "    best_params['random_forest'] = rf_params\n",
    "    print(f\"   En iyi CV F1 Skoru: {rf_score:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Optuna Optimizasyonu Tamamlandı ---\")\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba904ce7",
   "metadata": {},
   "source": [
    "### MAIN\n",
    "\n",
    "\"\"\"\n",
    "Ana işlem akışı - Karşılaştırmalı Analiz Pipeline\n",
    "\n",
    "4 Senaryo ile her tekniğin katkısını ölçer:\n",
    "1. Baseline (ham veri, varsayılan parametreler)\n",
    "2. Sadece SMOTE\n",
    "3. Sadece Feature Selection\n",
    "4. SMOTE + Feature Selection + Optuna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e041506",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ONLINE SHOPPERS INTENTION - KARŞILAŞTIRMALI ANALİZ\")\n",
    "print(\"SMOTE | Feature Selection | Optuna Hiperparametre Optimizasyonu\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Başlangıç zamanı: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Dosya yolları\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "dataset_path = os.path.join(script_dir, \"data\", \"online_shoppers_intention.csv\")\n",
    "output_dir = os.path.join(script_dir, \"results\")\n",
    "\n",
    "# Çıktı dizini oluştur\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nVeri seti: {dataset_path}\")\n",
    "print(f\"Çıktı dizini: {output_dir}\")\n",
    "\n",
    "# 1. Veri Yükleme\n",
    "try:\n",
    "    df = load_data(dataset_path)\n",
    "    print(f\"Veri boyutu: {df.shape[0]} satır, {df.shape[1]} sütun\")\n",
    "    print(f\"Hedef değişken dağılımı:\\n{df['Revenue'].value_counts()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Hata: {e}\")\n",
    "    return\n",
    "\n",
    "# 2. Ön İşleme\n",
    "df_processed = preprocess_data(df)\n",
    "\n",
    "# 3. Hipotez Testleri\n",
    "perform_hypothesis_testing(df_processed)\n",
    "\n",
    "# 4. Kümeleme (Analiz amaçlı, model eğitiminde kullanılmayacak)\n",
    "df_clustered = perform_clustering(df_processed)\n",
    "\n",
    "# 5. Veri Bölme\n",
    "X_train, X_test, y_train, y_test = get_data_splits(df_clustered)\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# ==========================================================================\n",
    "# 6. KARŞILAŞTIRMALI ANALİZ - 4 SENARYO\n",
    "# ==========================================================================\n",
    "results_df, scenario_best_models, best_params = run_comparative_analysis(\n",
    "    X_train, y_train, X_test, y_test, \n",
    "    feature_names, output_dir\n",
    ")\n",
    "\n",
    "# 7. Raporlama ve Görselleştirme\n",
    "summary_df, contributions = generate_comparison_report(\n",
    "    results_df, scenario_best_models, output_dir, best_params\n",
    ")\n",
    "\n",
    "# ==========================================================================\n",
    "# 8. SONUÇLARIN ÖZETİ\n",
    "# ==========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALİZ TAMAMLANDI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n📊 SENARYO KARŞILAŞTIRMA TABLOSU:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n📈 TEKNİKLERİN KATKI ANALİZİ:\")\n",
    "for technique, contrib in contributions.items():\n",
    "    f1_emoji = \"✅\" if contrib['F1'] > 0 else \"⚠️\"\n",
    "    acc_emoji = \"✅\" if contrib['Accuracy'] > 0 else \"⚠️\"\n",
    "    print(f\"  {technique}:\")\n",
    "    print(f\"    {f1_emoji} F1-Score: {contrib['F1']:+.2f}%\")\n",
    "    print(f\"    {acc_emoji} Accuracy: {contrib['Accuracy']:+.2f}%\")\n",
    "\n",
    "# En iyi sonucu belirle\n",
    "best_scenario = max(scenario_best_models.items(), key=lambda x: x[1]['F1-Score'])\n",
    "print(f\"\\n🏆 EN İYİ PERFORMANS:\")\n",
    "print(f\"   Senaryo: {best_scenario[0]}\")\n",
    "print(f\"   Model: {best_scenario[1]['Model']}\")\n",
    "print(f\"   Accuracy:  {best_scenario[1]['Accuracy']:.4f}\")\n",
    "print(f\"   Precision: {best_scenario[1]['Precision']:.4f}\")\n",
    "print(f\"   Recall:    {best_scenario[1]['Recall']:.4f}\")\n",
    "print(f\"   F1-Score:  {best_scenario[1]['F1-Score']:.4f}\")\n",
    "print(f\"   ROC AUC:   {best_scenario[1]['ROC AUC']:.4f}\")\n",
    "\n",
    "# Optuna parametreleri\n",
    "print(\"\\n🔧 OPTUNA İLE BULUNAN EN İYİ HİPERPARAMETRELER:\")\n",
    "for model_name, params in best_params.items():\n",
    "    print(f\"\\n   {model_name.upper()}:\")\n",
    "    for param, value in params.items():\n",
    "        print(f\"      - {param}: {value}\")\n",
    "\n",
    "print(f\"\\n📁 Çıktı dosyaları '{output_dir}' dizinine kaydedildi:\")\n",
    "print(\"   - comparison_results.csv (Tüm sonuçlar)\")\n",
    "print(\"   - scenario_summary.csv (Senaryo özetleri)\")\n",
    "print(\"   - comparison_barplot.png (Karşılaştırma grafiği)\")\n",
    "print(\"   - metrics_heatmap.png (Metrik heatmap)\")\n",
    "print(\"   - optuna_parameters.png (Parametre tablosu)\")\n",
    "\n",
    "print(f\"\\nBitiş zamanı: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "return results_df, summary_df, best_params"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
