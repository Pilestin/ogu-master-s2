# ğŸ‰ Veri Ã–niÅŸleme Scripti OluÅŸturuldu!

## ğŸ“¦ OluÅŸturulan Dosyalar

### 1. **data_preprocessing.py** (Ana Script)
KapsamlÄ± veri Ã¶niÅŸleme pipeline'Ä±. Åu Ã¶zellikleri iÃ§erir:

#### Ã–niÅŸleme Teknikleri:
- âœ… **Otomatik Kenar KÄ±rpma**: Gereksiz sÄ±nÄ±r bÃ¶lgelerini temizler
- âœ… **GÃ¼rÃ¼ltÃ¼ Azaltma**: 4 farklÄ± yÃ¶ntem (Bilateral, Gaussian, Median, NLM)
- âœ… **Kontrast Ä°yileÅŸtirme**: CLAHE, Histogram Equalization
- âœ… **KeskinleÅŸtirme**: DetaylarÄ± vurgula
- âœ… **Normalizasyon**: MinMax, Z-Score, Robust
- âœ… **Morfolojik Ä°ÅŸlemler**: Opening, Closing, Gradient
- âœ… **Veri ArtÄ±rma**: 6 farklÄ± augmentation tekniÄŸi

#### KonfigÃ¼rasyon Profilleri:
1. **Minimal**: HÄ±zlÄ±, temel iÅŸlemler
2. **Balanced**: Dengeli (Ã–NERILEN) â­
3. **Aggressive**: KapsamlÄ±, detaylÄ± iÅŸlemler

### 2. **demo_preprocessing.py** (HÄ±zlÄ± Test)
Tek bir gÃ¶rÃ¼ntÃ¼ Ã¼zerinde Ã¶niÅŸleme adÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirir.

### 3. **PREPROCESSING_GUIDE.md** (DetaylÄ± DokÃ¼mantasyon)
TÃ¼m tekniklerin aÃ§Ä±klamasÄ±, kullanÄ±m Ã¶rnekleri ve SSS.

### 4. **requirements.txt** (GÃ¼ncellendi)
Gerekli paketler eklendi: `scikit-image`, `tqdm`

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### AdÄ±m 1: Gerekli Paketleri YÃ¼kleyin
```bash
pip install -r requirements.txt
```

### AdÄ±m 2: HÄ±zlÄ± Demo (Opsiyonel)
```bash
python demo_preprocessing.py
```
Bu komut bir Ã¶rnek gÃ¶rÃ¼ntÃ¼ Ã¼zerinde adÄ±m adÄ±m Ã¶niÅŸleme gÃ¶sterir.

### AdÄ±m 3: Tam Ã–niÅŸleme
```bash
python data_preprocessing.py
```

Script interaktif olarak Ã§alÄ±ÅŸÄ±r:
1. Ã–rnek gÃ¶rÃ¼ntÃ¼ Ã¼zerinde Ã¶nizleme gÃ¶sterilir
2. 3 farklÄ± konfigÃ¼rasyon karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r
3. Tercih ettiÄŸiniz profili seÃ§ersiniz
4. Augmentation isteyip istemediÄŸinizi belirtirsiniz
5. TÃ¼m dataset iÅŸlenir ve kaydedilir

### AdÄ±m 4: Model Scriptlerini GÃ¼ncelleyin
Ã–niÅŸlenmiÅŸ verileri kullanmak iÃ§in model scriptlerinizde:

```python
# Eski
DATASET_ROOT = "dataset/wood"

# Yeni
DATASET_ROOT = "dataset/wood_preprocessed"
```

## ğŸ“Š Beklenen SonuÃ§lar

### Ã–ncesi (Orijinal)
- Dosya boyutu: ~2-5 MB/gÃ¶rÃ¼ntÃ¼
- GÃ¼rÃ¼ltÃ¼lÃ¼ kenarlar var
- DÃ¼ÅŸÃ¼k kontrast
- Veri sayÄ±sÄ±: 90 train + 10 test good + 36 test defect

### SonrasÄ± (Ã–niÅŸlenmiÅŸ)
- Dosya boyutu: ~100-500 KB/gÃ¶rÃ¼ntÃ¼ (daha kÃ¼Ã§Ã¼k!)
- Temiz kenarlar
- GeliÅŸtirilmiÅŸ kontrast
- Normalize edilmiÅŸ
- Veri sayÄ±sÄ±: 
  - Augmentation YOK ise: AynÄ±
  - Augmentation VAR ise: ~270-450 train (3-5x artÄ±ÅŸ)

### KayÄ±t Konumu
```
dataset/wood_preprocessed/
â”œâ”€â”€ train/good/                    # Ã–niÅŸlenmiÅŸ + augmented
â”œâ”€â”€ test/good/                     # Ã–niÅŸlenmiÅŸ
â”œâ”€â”€ test/defect/                   # Ã–niÅŸlenmiÅŸ
â””â”€â”€ preprocessing_config.json      # KullanÄ±lan ayarlar
```

## ğŸ’¡ Ã–nerilen KullanÄ±m

### Genel KullanÄ±m (Ã‡oÄŸu Proje Ä°Ã§in):
```
KonfigÃ¼rasyon: Balanced
Augmentation: Evet (eÄŸer training data < 100)
Target Size: 256x256
```

### HÄ±zlÄ± Test/Prototype:
```
KonfigÃ¼rasyon: Minimal
Augmentation: HayÄ±r
Target Size: 128x128
```

### Maksimum Kalite:
```
KonfigÃ¼rasyon: Aggressive
Augmentation: Evet
Target Size: 512x512
```

## ğŸ¯ Neden Bu Ã–niÅŸleme Gerekli?

### 1. Model PerformansÄ±nÄ± ArtÄ±rÄ±r
- GÃ¼rÃ¼ltÃ¼ azaltma â†’ Daha temiz Ã¶zellikler
- Kontrast iyileÅŸtirme â†’ Defektler daha belirgin
- Normalizasyon â†’ Stabil eÄŸitim

### 2. EÄŸitim SÃ¼resini AzaltÄ±r
- KÃ¼Ã§Ã¼k dosya boyutu â†’ HÄ±zlÄ± yÃ¼kleme
- Standart boyut â†’ Batch iÅŸleme kolaylÄ±ÄŸÄ±

### 3. Generalizasyonu Ä°yileÅŸtirir
- Augmentation â†’ Daha fazla Ã§eÅŸitlilik
- Normalizasyon â†’ Overfitting azalÄ±r

### 4. Gereksiz Bilgiyi Temizler
- Kenar kÄ±rpma â†’ Sadece ilgili bÃ¶lge
- Morfolojik iÅŸlemler â†’ KÃ¼Ã§Ã¼k gÃ¼rÃ¼ltÃ¼ler kaybolur

## ğŸ“š Ek Kaynaklar

- **DetaylÄ± DokÃ¼mantasyon**: `PREPROCESSING_GUIDE.md`
- **Teknik Referanslar**: PREPROCESSING_GUIDE.md â†’ Referanslar bÃ¶lÃ¼mÃ¼
- **Troubleshooting**: PREPROCESSING_GUIDE.md â†’ Troubleshooting bÃ¶lÃ¼mÃ¼

## ğŸ” GÃ¶rselleÅŸtirme

Script Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda otomatik olarak ÅŸunlarÄ± gÃ¶sterir:

1. **AdÄ±m AdÄ±m Ä°ÅŸleme**:
   - Orijinal gÃ¶rÃ¼ntÃ¼
   - Her Ã¶niÅŸleme adÄ±mÄ±nÄ±n etkisi
   - Final sonuÃ§

2. **KonfigÃ¼rasyon KarÅŸÄ±laÅŸtÄ±rma**:
   - Minimal vs Balanced vs Aggressive
   - Yan yana gÃ¶rsel karÅŸÄ±laÅŸtÄ±rma

3. **Ä°statistikler**:
   ```
   Dataset Ä°ÅŸleme Ã–zeti:
   âœ“ Train Good: 90 gÃ¶rÃ¼ntÃ¼
   âœ“ Train Good (Augmented): 270 ek gÃ¶rÃ¼ntÃ¼
   âœ“ Toplam Train: 360 gÃ¶rÃ¼ntÃ¼
   âœ“ Test Good: 10 gÃ¶rÃ¼ntÃ¼
   âœ“ Test Defect: 36 gÃ¶rÃ¼ntÃ¼
   ```

## âš™ï¸ Ä°leri Seviye KullanÄ±m

### Ã–zel KonfigÃ¼rasyon

Python scriptinde doÄŸrudan kullanÄ±m:

```python
from data_preprocessing import ImagePreprocessor, process_and_save_dataset

# Ã–zel konfigÃ¼rasyon
custom_config = {
    'auto_crop': True,
    'denoise': 'bilateral',
    'contrast': 'clahe',
    'sharpen': True,
    'sharpen_strength': 0.7,
    'normalize': 'robust',
    'morphology': 'opening',
    'resize': True
}

# Ã–zel boyut
preprocessor = ImagePreprocessor(target_size=(512, 512))

# Ä°ÅŸle
process_and_save_dataset(preprocessor, custom_config, apply_augmentation=True)
```

### Tek GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme

```python
from data_preprocessing import ImagePreprocessor

preprocessor = ImagePreprocessor(target_size=(256, 256))
image = preprocessor.load_image("path/to/image.bmp")
processed = preprocessor.preprocess_pipeline(image)

import cv2
cv2.imwrite("output.png", cv2.cvtColor(processed, cv2.COLOR_RGB2BGR))
```

## ğŸ› YaygÄ±n Hatalar ve Ã‡Ã¶zÃ¼mleri

### Hata: "ModuleNotFoundError: No module named 'skimage'"
**Ã‡Ã¶zÃ¼m**:
```bash
pip install scikit-image
```

### Hata: "FileNotFoundError: dataset/wood/train/good"
**Ã‡Ã¶zÃ¼m**: Dataset klasÃ¶r yapÄ±sÄ±nÄ± kontrol edin:
```
dataset/
â””â”€â”€ wood/
    â”œâ”€â”€ train/
    â”‚   â””â”€â”€ good/  â† BMP dosyalarÄ± burada
    â””â”€â”€ test/
        â”œâ”€â”€ good/
        â””â”€â”€ defect/
```

### Hata: GÃ¶rÃ¼ntÃ¼ler yÃ¼klenemiyor
**Ã‡Ã¶zÃ¼m**: BMP formatÄ±nda olmadÄ±ÄŸÄ± iÃ§in olabilir. Script'te `.bmp` yerine:
```python
[f for f in os.listdir(path) if f.endswith(('.bmp', '.png', '.jpg'))]
```

## ğŸ“ˆ Performans Beklentileri

### Ä°ÅŸlem SÃ¼resi (90 train + 46 test gÃ¶rÃ¼ntÃ¼ iÃ§in):
- **Minimal**: ~10-15 saniye
- **Balanced**: ~30-45 saniye â­
- **Aggressive**: ~2-3 dakika

### Bellek KullanÄ±mÄ±:
- RAM: ~500 MB
- Disk (Ã§Ä±ktÄ±): ~50-200 MB

## âœ… Checklist

Ã–niÅŸleme yapmadan Ã¶nce:

- [ ] Dataset klasÃ¶rÃ¼ doÄŸru konumda
- [ ] Gerekli paketler yÃ¼klendi (`pip install -r requirements.txt`)
- [ ] Yeterli disk alanÄ± var (~200 MB)
- [ ] Orijinal veriler yedeklendi (Ã¶nerilir)

Ã–niÅŸleme sonrasÄ±:

- [ ] `dataset/wood_preprocessed/` klasÃ¶rÃ¼ oluÅŸtu
- [ ] GÃ¶rÃ¼ntÃ¼ler baÅŸarÄ±yla kaydedildi
- [ ] `preprocessing_config.json` oluÅŸturuldu
- [ ] Model scriptlerinde `DATASET_ROOT` gÃ¼ncellendi

## ğŸ“ SonuÃ§

ArtÄ±k elinizde profesyonel bir veri Ã¶niÅŸleme pipeline'Ä± var! Bu script:

âœ… LiteratÃ¼rde kanÄ±tlanmÄ±ÅŸ teknikleri kullanÄ±r
âœ… Esnek ve Ã¶zelleÅŸtirilebilir
âœ… Ä°nteraktif ve kullanÄ±mÄ± kolay
âœ… GÃ¶rselleÅŸtirme ile sonuÃ§larÄ± gÃ¶sterir
âœ… KapsamlÄ± dokÃ¼mantasyon ile desteklenir

**Ä°yi Ã§alÄ±ÅŸmalar!** ğŸš€

---

**Sorular?** 
- DetaylÄ± bilgi iÃ§in: `PREPROCESSING_GUIDE.md`
- Hata durumunda: PREPROCESSING_GUIDE.md â†’ Troubleshooting
