# Veri Ã–niÅŸleme Rehberi

## ğŸ“‹ Ä°Ã§indekiler
1. [Genel BakÄ±ÅŸ](#genel-bakÄ±ÅŸ)
2. [Ã–niÅŸleme Teknikleri](#Ã¶niÅŸleme-teknikleri)
3. [KullanÄ±m](#kullanÄ±m)
4. [KonfigÃ¼rasyon SeÃ§enekleri](#konfigÃ¼rasyon-seÃ§enekleri)
5. [Performans Ä°puÃ§larÄ±](#performans-ipuÃ§larÄ±)

## ğŸ¯ Genel BakÄ±ÅŸ

`data_preprocessing.py` scripti, ahÅŸap yÃ¼zey anomali tespiti iÃ§in optimize edilmiÅŸ kapsamlÄ± bir gÃ¶rÃ¼ntÃ¼ Ã¶niÅŸleme pipeline'Ä± saÄŸlar. Bu script literatÃ¼rde yaygÄ±n kullanÄ±lan teknikleri iÃ§erir ve model performansÄ±nÄ± artÄ±rmak iÃ§in tasarlanmÄ±ÅŸtÄ±r.

### Neden Veri Ã–niÅŸleme?

- **GÃ¼rÃ¼ltÃ¼ Azaltma**: GÃ¶rÃ¼ntÃ¼lerdeki istenmeyen pikselleri temizler
- **Normalizasyon**: Model eÄŸitimini stabilize eder
- **Kontrast Ä°yileÅŸtirme**: Defekt bÃ¶lgelerini daha belirgin hale getirir
- **Gereksiz BÃ¶lge Temizleme**: Kenar bÃ¶lgelerindeki anlamsÄ±z pikselleri kaldÄ±rÄ±r
- **Veri ArtÄ±rma**: Az sayÄ±da eÄŸitim verisini Ã§oÄŸaltÄ±r

## ğŸ”§ Ã–niÅŸleme Teknikleri

### 1. Otomatik Kenar KÄ±rpma (Auto Crop)
**AmaÃ§**: GÃ¶rÃ¼ntÃ¼nÃ¼n kenarlarÄ±ndaki gereksiz boÅŸ/uniform bÃ¶lgeleri kaldÄ±rÄ±r.

**NasÄ±l Ã‡alÄ±ÅŸÄ±r**:
- Canny edge detection ile kenarlar tespit edilir
- Ä°Ã§erik iÃ§eren minimum bounding box hesaplanÄ±r
- Gereksiz kenarlar otomatik olarak kÄ±rpÄ±lÄ±r

**Ne Zaman KullanÄ±lÄ±r**: AhÅŸap gÃ¶rÃ¼ntÃ¼lerinde sÄ±kÃ§a gÃ¶rÃ¼len kenar gÃ¼rÃ¼ltÃ¼sÃ¼ iÃ§in Ã¶nerilir.

### 2. GÃ¼rÃ¼ltÃ¼ Azaltma (Denoising)

#### Bilateral Filter (Ã–nerilen)
- **Ã–zellik**: KenarlarÄ± koruyarak gÃ¼rÃ¼ltÃ¼ azaltÄ±r
- **Avantaj**: DetaylarÄ± korur, defekt bÃ¶lgelerini bulanÄ±klaÅŸtÄ±rmaz
- **HÄ±z**: Orta

#### Gaussian Blur
- **Ã–zellik**: Uniform bulanÄ±klÄ±k
- **Avantaj**: HÄ±zlÄ± iÅŸlem
- **Dezavantaj**: KenarlarÄ± da bulanÄ±klaÅŸtÄ±rÄ±r

#### Median Filter
- **Ã–zellik**: Salt-and-pepper gÃ¼rÃ¼ltÃ¼sÃ¼ iÃ§in ideal
- **Avantaj**: Outlier'lara karÅŸÄ± dayanÄ±klÄ±
- **HÄ±z**: Orta-YavaÅŸ

#### Non-Local Means (NLM)
- **Ã–zellik**: En geliÅŸmiÅŸ gÃ¼rÃ¼ltÃ¼ azaltma
- **Avantaj**: En iyi sonuÃ§
- **Dezavantaj**: Ã‡ok yavaÅŸ

### 3. Kontrast Ä°yileÅŸtirme

#### CLAHE (Contrast Limited Adaptive Histogram Equalization) - Ã–nerilen
- Yerel kontrastÄ± iyileÅŸtirir
- AÅŸÄ±rÄ± kontrast artÄ±ÅŸÄ±nÄ± Ã¶nler
- Defekt bÃ¶lgelerini vurgular

#### Global Histogram Equalization
- TÃ¼m gÃ¶rÃ¼ntÃ¼ye uniform uygulama
- Basit ve hÄ±zlÄ±

#### Adaptive Histogram Equalization
- Scikit-image tabanlÄ±
- Daha yumuÅŸak sonuÃ§lar

### 4. KeskinleÅŸtirme (Sharpening)
- GÃ¶rÃ¼ntÃ¼ detaylarÄ±nÄ± vurgular
- Defekt kenarlarÄ±nÄ± belirginleÅŸtirir
- Ayarlanabilir strength parametresi (0.5-1.0 Ã¶nerilir)

### 5. Normalizasyon

#### Min-Max Normalization (Ã–nerilen)
```
normalized = (pixel - min) / (max - min)
```
- [0, 1] aralÄ±ÄŸÄ±na getirir
- En yaygÄ±n kullanÄ±lan yÃ¶ntem

#### Z-Score Normalization
```
normalized = (pixel - mean) / std
```
- Mean=0, Std=1 daÄŸÄ±lÄ±mÄ±
- Ä°statistiksel normalleÅŸtirme

#### Robust Normalization
```
normalized = (pixel - p2) / (p98 - p2)
```
- Outlier'lara karÅŸÄ± dayanÄ±klÄ±
- AykÄ±rÄ± deÄŸerleri gÃ¶rmezden gelir

### 6. Morfolojik Ä°ÅŸlemler

#### Opening
- KÃ¼Ã§Ã¼k gÃ¼rÃ¼ltÃ¼leri kaldÄ±rÄ±r
- Ã–nce erosion, sonra dilation

#### Closing
- KÃ¼Ã§Ã¼k delikleri kapatÄ±r
- Ã–nce dilation, sonra erosion

#### Gradient
- KenarlarÄ± vurgular
- Dilation - erosion farkÄ±

### 7. Veri ArtÄ±rma (Data Augmentation)

**Training data iÃ§in kullanÄ±lÄ±r, test data iÃ§in uygulanmaz!**

Desteklenen iÅŸlemler:
- âœ… Yatay Ã§evirme (Horizontal flip)
- âœ… Dikey Ã§evirme (Vertical flip)
- âœ… 90Â°, 180Â°, 270Â° dÃ¶ndÃ¼rme
- âœ… ParlaklÄ±k ayarlama
- âœ… Gaussian gÃ¼rÃ¼ltÃ¼ ekleme
- âœ… Rastgele kÄ±rpma

## ğŸ’» KullanÄ±m

### Temel KullanÄ±m

```bash
python data_preprocessing.py
```

Script interaktif olarak Ã§alÄ±ÅŸÄ±r ve aÅŸaÄŸÄ±daki adÄ±mlarÄ± izler:

1. **Ã–nizleme**: Ã–rnek gÃ¶rÃ¼ntÃ¼ Ã¼zerinde adÄ±m adÄ±m Ã¶niÅŸleme gÃ¶sterilir
2. **KarÅŸÄ±laÅŸtÄ±rma**: FarklÄ± konfigÃ¼rasyonlar karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r
3. **KonfigÃ¼rasyon SeÃ§imi**: Ä°stediÄŸiniz profili seÃ§ersiniz
4. **Augmentation SeÃ§imi**: Training data iÃ§in augmentation isteyip istemediÄŸinizi belirtirsiniz
5. **Ä°ÅŸleme**: TÃ¼m dataset iÅŸlenir ve kaydedilir

### HÄ±zlÄ± Demo

```bash
python demo_preprocessing.py
```

Sadece bir Ã¶rnek gÃ¶rÃ¼ntÃ¼ Ã¼zerinde Ã¶niÅŸleme adÄ±mlarÄ±nÄ± gÃ¶rselleÅŸtirir.

## âš™ï¸ KonfigÃ¼rasyon SeÃ§enekleri

### 1. Minimal (HÄ±zlÄ±)
```python
{
    'auto_crop': True,
    'denoise': None,
    'contrast': None,
    'sharpen': False,
    'normalize': 'minmax',
    'resize': True
}
```
- **KullanÄ±m**: Ä°lk testler, hÄ±zlÄ± denemeler
- **Ä°ÅŸlem SÃ¼resi**: En hÄ±zlÄ±
- **Kalite**: Temel

### 2. Balanced (Ã–nerilen)
```python
{
    'auto_crop': True,
    'denoise': 'bilateral',
    'contrast': 'clahe',
    'sharpen': True,
    'sharpen_strength': 0.5,
    'normalize': 'minmax',
    'resize': True
}
```
- **KullanÄ±m**: Genel amaÃ§lÄ±, production
- **Ä°ÅŸlem SÃ¼resi**: Orta
- **Kalite**: YÃ¼ksek
- **En iyi seÃ§im!**

### 3. Aggressive (DetaylÄ±)
```python
{
    'auto_crop': True,
    'denoise': 'nlm',
    'contrast': 'clahe',
    'sharpen': True,
    'sharpen_strength': 1.0,
    'normalize': 'robust',
    'morphology': 'opening',
    'resize': True
}
```
- **KullanÄ±m**: Maksimum kalite gerektiÄŸinde
- **Ä°ÅŸlem SÃ¼resi**: YavaÅŸ
- **Kalite**: En yÃ¼ksek

### Ã–zel KonfigÃ¼rasyon

Python kodunda kendi konfigÃ¼rasyonunuzu oluÅŸturabilirsiniz:

```python
from data_preprocessing import ImagePreprocessor, process_and_save_dataset

custom_config = {
    'auto_crop': True,
    'denoise': 'bilateral',
    'contrast': 'clahe',
    'sharpen': True,
    'sharpen_strength': 0.7,
    'normalize': 'robust',
    'morphology': 'closing',
    'resize': True
}

preprocessor = ImagePreprocessor(target_size=(512, 512))
process_and_save_dataset(preprocessor, custom_config, apply_augmentation=True)
```

## ğŸš€ Performans Ä°puÃ§larÄ±

### Ä°ÅŸlem SÃ¼resi Optimizasyonu

1. **KÃ¼Ã§Ã¼k GÃ¶rÃ¼ntÃ¼ Boyutu**: 
   - 256x256 yerine 128x128 kullanÄ±n (4x hÄ±zlÄ±)
   - Ancak model performansÄ± dÃ¼ÅŸebilir

2. **Denoising SeÃ§imi**:
   - En HÄ±zlÄ±: Gaussian > Median > Bilateral > NLM (En YavaÅŸ)
   - Balanced profile zaten optimal

3. **Augmentation**:
   - Sadece training data iÃ§in kullanÄ±n
   - 3-5 augmentation yeterli (daha fazlasÄ± gereksiz)

### Bellek KullanÄ±mÄ±

- BÃ¼yÃ¼k dataset'ler iÃ§in batch iÅŸleme otomatik yapÄ±lÄ±r
- TÃ¼m gÃ¶rÃ¼ntÃ¼ler aynÄ± anda yÃ¼klenmez
- Tipik kullanÄ±m: ~500MB RAM

### Disk AlanÄ±

- Orijinal: ~2-5 MB/gÃ¶rÃ¼ntÃ¼ (BMP formatÄ±)
- Ã–niÅŸlenmiÅŸ: ~100-500 KB/gÃ¶rÃ¼ntÃ¼ (daha kÃ¼Ã§Ã¼k boyut)
- Augmentation ile ~3-5x artÄ±ÅŸ

## ğŸ“Š SonuÃ§lar

Ã–niÅŸlenmiÅŸ veriler ÅŸu klasÃ¶re kaydedilir:

```
dataset/wood_preprocessed/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ good/                  # Ã–niÅŸlenmiÅŸ + augmented (opsiyonel)
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ good/                  # Ã–niÅŸlenmiÅŸ
â”‚   â””â”€â”€ defect/                # Ã–niÅŸlenmiÅŸ
â””â”€â”€ preprocessing_config.json  # KullanÄ±lan konfigÃ¼rasyon
```

### Model Scriptlerini GÃ¼ncelleme

Ã–niÅŸlenmiÅŸ verileri kullanmak iÃ§in model scriptlerinizde ÅŸu deÄŸiÅŸikliÄŸi yapÄ±n:

```python
# Eski
DATASET_ROOT = "dataset/wood"

# Yeni
DATASET_ROOT = "dataset/wood_preprocessed"
```

## ğŸ” GÃ¶rselleÅŸtirme

Script otomatik olarak ÅŸunlarÄ± gÃ¶sterir:

1. **AdÄ±m AdÄ±m Ä°ÅŸleme**: Her Ã¶niÅŸleme adÄ±mÄ±nÄ±n etkisi
2. **KonfigÃ¼rasyon KarÅŸÄ±laÅŸtÄ±rma**: 3 profil yan yana
3. **Ä°statistikler**: Ä°ÅŸlenen dosya sayÄ±larÄ±

## â“ SSS

### S: Hangi konfigÃ¼rasyonu seÃ§meliyim?
**C**: Balanced profile Ã§oÄŸu durumda en iyi seÃ§imdir. EÄŸer sonuÃ§lar yeterince iyi deÄŸilse Aggressive'i deneyin.

### S: Augmentation kullanmalÄ± mÄ±yÄ±m?
**C**: EÄŸer training veriniz az ise (100'den az), kesinlikle evet. Dataset'iniz yeterince bÃ¼yÃ¼kse gerekmeyebilir.

### S: Ä°ÅŸlem ne kadar sÃ¼rer?
**C**: 
- Minimal: ~10-15 saniye (100 gÃ¶rÃ¼ntÃ¼)
- Balanced: ~30-45 saniye
- Aggressive: ~2-3 dakika

### S: Orijinal verileri silebilir miyim?
**C**: HayÄ±r! Her zaman orijinal verileri saklayÄ±n. FarklÄ± konfigÃ¼rasyonlarÄ± test etmek isteyebilirsiniz.

### S: Ã–zel bir boyut kullanabilir miyim?
**C**: Evet, kodu dÃ¼zenleyerek `target_size` parametresini deÄŸiÅŸtirebilirsiniz.

## ğŸ“š Referanslar

KullanÄ±lan teknikler ÅŸu kaynaklara dayanÄ±r:

1. **CLAHE**: Zuiderveld, K. (1994). "Contrast Limited Adaptive Histogram Equalization"
2. **Bilateral Filter**: Tomasi, C., & Manduchi, R. (1998). "Bilateral filtering for gray and color images"
3. **Non-Local Means**: Buades, A., et al. (2005). "A non-local algorithm for image denoising"
4. **Data Augmentation**: Shorten, C., & Khoshgoftaar, T. M. (2019). "A survey on image data augmentation"

## ğŸ› ï¸ Troubleshooting

### Hata: ModuleNotFoundError
```bash
pip install -r requirements.txt
```

### Hata: Dataset bulunamadÄ±
KlasÃ¶r yapÄ±sÄ±nÄ± kontrol edin:
```
dataset/wood/train/good/  â†’ BMP dosyalarÄ± burada olmalÄ±
```

### Hata: GÃ¶rÃ¼ntÃ¼ yÃ¼klenemiyor
BMP formatÄ±nda olduÄŸundan emin olun. DiÄŸer formatlar iÃ§in kodu gÃ¼ncelleyin:
```python
# .bmp yerine .png veya .jpg
train_files = [f for f in os.listdir(TRAIN_GOOD_PATH) if f.endswith(('.bmp', '.png', '.jpg'))]
```

---

**Not**: SorularÄ±nÄ±z iÃ§in GitHub Issues kullanabilir veya doÄŸrudan iletiÅŸime geÃ§ebilirsiniz.
