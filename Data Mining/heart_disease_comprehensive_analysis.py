"""
UCI Kalp HastalÄ±ÄŸÄ± Veri Seti - KapsamlÄ± Ä°statistiksel Analiz ve GÃ¶rselleÅŸtirme
================================================================================

Bu script, UCI Heart Disease veri setinin detaylÄ± analizini gerÃ§ekleÅŸtirir:
- Veri yapÄ±sÄ± ve kalite analizi
- TanÄ±mlayÄ±cÄ± istatistikler
- Outlier (aykÄ±rÄ± deÄŸer) tespiti
- Korelasyon analizi
- Kategorik deÄŸiÅŸken analizleri
- Ä°statistiksel testler
- KapsamlÄ± gÃ¶rselleÅŸtirmeler

Tarih: AralÄ±k 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import chi2_contingency, shapiro, normaltest, kstest
import warnings
warnings.filterwarnings('ignore')

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 10
plt.rcParams['axes.unicode_minus'] = False

print("="*80)
print("UCI KALP HASTALIÄI VERÄ° SETÄ° - KAPSAMLI Ä°STATÄ°STÄ°KSEL ANALÄ°Z")
print("="*80)

# ============================================================================
# 1. VERÄ° YÃœKLEME VE GENEL BAKIÅ
# ============================================================================

def load_and_explore_data(file_path):
    """
    Veri setini yÃ¼kler ve temel bilgileri gÃ¶sterir.
    """
    print("\n" + "="*80)
    print("1. VERÄ° YÃœKLEME VE GENEL BAKIÅ")
    print("="*80)
    
    df = pd.read_csv(file_path)
    
    # Sadece Cleveland verisini filtrele
    if 'dataset' in df.columns:
        print(f"\nğŸ” Veri seti filtreleme: Sadece Cleveland verileri kullanÄ±lÄ±yor...")
        original_count = len(df)
        df = df[df['dataset'] == 'Cleveland'].copy()
        print(f"   â€¢ Orijinal kayÄ±t sayÄ±sÄ±: {original_count}")
        print(f"   â€¢ Cleveland kayÄ±t sayÄ±sÄ±: {len(df)}")
        print(f"   â€¢ Filtrelenen kayÄ±t: {original_count - len(df)}")
    
    print(f"\nğŸ“Š Veri Seti BoyutlarÄ±:")
    print(f"   â€¢ Toplam kayÄ±t sayÄ±sÄ±: {df.shape[0]}")
    print(f"   â€¢ Ã–zellik sayÄ±sÄ±: {df.shape[1]}")
    
    print(f"\nğŸ“‹ Ä°lk 10 SatÄ±r:")
    print(df.head(10))
    
    print(f"\nğŸ” Veri Tipleri ve Genel Bilgi:")
    print(df.info())
    
    print(f"\nğŸ“ˆ SÃ¼tun AdlarÄ±:")
    print(df.columns.tolist())
    
    return df

# ============================================================================
# 2. EKSÄ°K DEÄER VE VERÄ° KALÄ°TESÄ° ANALÄ°ZÄ°
# ============================================================================

def analyze_data_quality(df):
    """
    Eksik deÄŸerleri ve veri kalitesini analiz eder.
    """
    print("\n" + "="*80)
    print("2. VERÄ° KALÄ°TESÄ° VE EKSÄ°K DEÄER ANALÄ°ZÄ°")
    print("="*80)
    
    # Eksik deÄŸer analizi
    missing_count = df.isnull().sum()
    missing_percent = (missing_count / len(df)) * 100
    
    missing_df = pd.DataFrame({
        'SÃ¼tun': missing_count.index,
        'Eksik SayÄ±': missing_count.values,
        'Eksik %': missing_percent.values
    }).sort_values('Eksik SayÄ±', ascending=False)
    
    print("\nğŸ” Eksik DeÄŸer Raporu:")
    print(missing_df)
    
    total_missing = missing_count.sum()
    total_cells = df.shape[0] * df.shape[1]
    
    print(f"\nğŸ“Š Ã–zet:")
    print(f"   â€¢ Toplam eksik deÄŸer: {total_missing}")
    print(f"   â€¢ Toplam hÃ¼cre sayÄ±sÄ±: {total_cells}")
    print(f"   â€¢ Eksik deÄŸer oranÄ±: {(total_missing/total_cells)*100:.2f}%")
    
    # Veri tipi analizi
    print("\nğŸ“‹ Veri Tipi DaÄŸÄ±lÄ±mÄ±:")
    print(f"   â€¢ SayÄ±sal (numeric): {len(df.select_dtypes(include=[np.number]).columns)}")
    print(f"   â€¢ Kategorik (object): {len(df.select_dtypes(include=['object']).columns)}")
    
    # Benzersiz deÄŸer sayÄ±larÄ±
    print("\nğŸ”¢ Her SÃ¼tundaki Benzersiz DeÄŸer SayÄ±larÄ±:")
    for col in df.columns:
        unique_count = df[col].nunique()
        unique_percent = (unique_count / len(df)) * 100
        print(f"   â€¢ {col:15s}: {unique_count:4d} benzersiz deÄŸer ({unique_percent:5.1f}%)")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Eksik deÄŸer Ä±sÄ± haritasÄ±
    if missing_count.sum() > 0:
        missing_data = df.isnull()
        sns.heatmap(missing_data, yticklabels=False, cbar=True, cmap='YlOrRd', ax=axes[0])
        axes[0].set_title('Eksik DeÄŸer IsÄ± HaritasÄ±', fontsize=14, fontweight='bold')
    else:
        axes[0].text(0.5, 0.5, 'Eksik deÄŸer yok!', 
                    ha='center', va='center', fontsize=16, fontweight='bold')
        axes[0].set_title('Eksik DeÄŸer Analizi', fontsize=14, fontweight='bold')
    
    # Veri tipi daÄŸÄ±lÄ±mÄ±
    dtype_counts = df.dtypes.value_counts()
    axes[1].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%', startangle=90)
    axes[1].set_title('Veri Tipi DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('img/01_data_quality_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return missing_df

# ============================================================================
# 3. TANIMLAYICI Ä°STATÄ°STÄ°KLER
# ============================================================================

def descriptive_statistics(df):
    """
    DetaylÄ± tanÄ±mlayÄ±cÄ± istatistikler hesaplar.
    """
    print("\n" + "="*80)
    print("3. TANIMLAYICI Ä°STATÄ°STÄ°KLER")
    print("="*80)
    
    # SayÄ±sal deÄŸiÅŸkenler
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'id' in numeric_cols:
        numeric_cols.remove('id')
    
    print("\nğŸ“Š SayÄ±sal DeÄŸiÅŸkenler - Temel Ä°statistikler:")
    stats_df = df[numeric_cols].describe()
    print(stats_df)
    
    # Ek istatistikler
    print("\nğŸ“ˆ Ek Ä°statistiksel Ã–lÃ§Ã¼mler:")
    additional_stats = pd.DataFrame({
        'Ã‡arpÄ±klÄ±k (Skewness)': df[numeric_cols].skew(),
        'BasÄ±klÄ±k (Kurtosis)': df[numeric_cols].kurtosis(),
        'Medyan': df[numeric_cols].median(),
        'Mod': df[numeric_cols].mode().iloc[0] if len(df[numeric_cols].mode()) > 0 else np.nan,
        'Varyans': df[numeric_cols].var()
    })
    print(additional_stats)
    
    # Kategorik deÄŸiÅŸkenler
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    if 'id' in categorical_cols:
        categorical_cols.remove('id')
    
    print("\nğŸ“‹ Kategorik DeÄŸiÅŸkenler - Frekans Analizi:")
    for col in categorical_cols:
        print(f"\n{col}:")
        freq_table = df[col].value_counts()
        freq_percent = df[col].value_counts(normalize=True) * 100
        freq_df = pd.DataFrame({
            'Frekans': freq_table,
            'YÃ¼zde (%)': freq_percent
        })
        print(freq_df)
    
    # Hedef deÄŸiÅŸken (num) analizi
    if 'num' in df.columns:
        print("\nğŸ¯ Hedef DeÄŸiÅŸken (num - Kalp HastalÄ±ÄŸÄ± Derecesi) DaÄŸÄ±lÄ±mÄ±:")
        target_dist = df['num'].value_counts().sort_index()
        target_percent = df['num'].value_counts(normalize=True).sort_index() * 100
        target_df = pd.DataFrame({
            'Frekans': target_dist,
            'YÃ¼zde (%)': target_percent
        })
        print(target_df)
    
    # GÃ¶rselleÅŸtirme
    n_numeric = len(numeric_cols)
    n_rows = (n_numeric + 2) // 3
    
    fig, axes = plt.subplots(n_rows, 3, figsize=(18, n_rows * 5))
    axes = axes.ravel() if n_numeric > 1 else [axes]
    
    for idx, col in enumerate(numeric_cols):
        axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7, color='steelblue')
        axes[idx].set_title(f'{col} DaÄŸÄ±lÄ±mÄ±', fontsize=12, fontweight='bold')
        axes[idx].set_xlabel(col)
        axes[idx].set_ylabel('Frekans')
        axes[idx].axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Ortalama: {df[col].mean():.2f}')
        axes[idx].axvline(df[col].median(), color='green', linestyle='--', linewidth=2, label=f'Medyan: {df[col].median():.2f}')
        axes[idx].legend()
        axes[idx].grid(True, alpha=0.3)
    
    # KullanÄ±lmayan subplot'larÄ± gizle
    for idx in range(n_numeric, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('img/02_descriptive_statistics_distributions.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return stats_df, additional_stats

# ============================================================================
# 4. OUTLIER (AYKIRI DEÄER) ANALÄ°ZÄ°
# ============================================================================

def detect_outliers(df):
    """
    IQR ve Z-score yÃ¶ntemleri ile outlier tespiti yapar.
    """
    print("\n" + "="*80)
    print("4. OUTLIER (AYKIRI DEÄER) ANALÄ°ZÄ°")
    print("="*80)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'id' in numeric_cols:
        numeric_cols.remove('id')
    
    outlier_summary = {}
    
    print("\nğŸ” IQR YÃ¶ntemi ile Outlier Tespiti:")
    print("-" * 80)
    
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        outlier_count = len(outliers)
        outlier_percent = (outlier_count / len(df)) * 100
        
        outlier_summary[col] = {
            'count': outlier_count,
            'percent': outlier_percent,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'Q1': Q1,
            'Q3': Q3,
            'IQR': IQR
        }
        
        print(f"\n{col}:")
        print(f"   â€¢ Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}")
        print(f"   â€¢ Alt sÄ±nÄ±r: {lower_bound:.2f}, Ãœst sÄ±nÄ±r: {upper_bound:.2f}")
        print(f"   â€¢ Outlier sayÄ±sÄ±: {outlier_count} ({outlier_percent:.1f}%)")
        if outlier_count > 0:
            print(f"   â€¢ Outlier deÄŸerleri: {outliers[col].values}")
    
    # Z-score yÃ¶ntemi
    print("\n" + "-" * 80)
    print("ğŸ“Š Z-Score YÃ¶ntemi ile Outlier Tespiti (|Z| > 3):")
    print("-" * 80)
    
    z_outlier_summary = {}
    for col in numeric_cols:
        z_scores = np.abs(stats.zscore(df[col].dropna()))
        outlier_mask = z_scores > 3
        outlier_count = outlier_mask.sum()
        outlier_percent = (outlier_count / len(df)) * 100
        
        z_outlier_summary[col] = {
            'count': outlier_count,
            'percent': outlier_percent
        }
        
        print(f"{col:15s}: {outlier_count:3d} outlier ({outlier_percent:5.2f}%)")
    
    # GÃ¶rselleÅŸtirme - Box plots
    n_cols = len(numeric_cols)
    n_rows = (n_cols + 2) // 3
    
    fig, axes = plt.subplots(n_rows, 3, figsize=(18, n_rows * 5))
    axes = axes.ravel() if n_cols > 1 else [axes]
    
    for idx, col in enumerate(numeric_cols):
        # Box plot
        box_parts = axes[idx].boxplot(df[col].dropna(), vert=True, patch_artist=True,
                                       labels=[col],
                                       boxprops=dict(facecolor='lightblue', alpha=0.7),
                                       medianprops=dict(color='red', linewidth=2),
                                       whiskerprops=dict(color='blue', linewidth=1.5),
                                       capprops=dict(color='blue', linewidth=1.5),
                                       flierprops=dict(marker='o', markerfacecolor='red', 
                                                      markersize=8, alpha=0.5))
        
        axes[idx].set_title(f'{col} - Box Plot\n(Outliers: {outlier_summary[col]["count"]})', 
                           fontsize=12, fontweight='bold')
        axes[idx].set_ylabel('DeÄŸer')
        axes[idx].grid(True, alpha=0.3)
        
        # Outlier bilgilerini ekle
        textstr = f'Q1: {outlier_summary[col]["Q1"]:.2f}\n'
        textstr += f'Q3: {outlier_summary[col]["Q3"]:.2f}\n'
        textstr += f'IQR: {outlier_summary[col]["IQR"]:.2f}'
        axes[idx].text(0.98, 0.98, textstr, transform=axes[idx].transAxes,
                      verticalalignment='top', horizontalalignment='right',
                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                      fontsize=9)
    
    # KullanÄ±lmayan subplot'larÄ± gizle
    for idx in range(n_cols, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('img/03_outlier_analysis_boxplots.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Outlier Ã¶zet tablosu
    outlier_summary_df = pd.DataFrame(outlier_summary).T
    print("\nğŸ“‹ Outlier Ã–zet Tablosu:")
    print(outlier_summary_df[['count', 'percent', 'lower_bound', 'upper_bound']])
    
    return outlier_summary, z_outlier_summary

# ============================================================================
# 5. KORELASYON ANALÄ°ZÄ°
# ============================================================================

def correlation_analysis(df):
    """
    DeÄŸiÅŸkenler arasÄ± korelasyon analizi yapar.
    """
    print("\n" + "="*80)
    print("5. KORELASYON ANALÄ°ZÄ°")
    print("="*80)
    
    # SayÄ±sal deÄŸiÅŸkenler iÃ§in korelasyon
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'id' in numeric_cols:
        numeric_cols.remove('id')
    
    correlation_matrix = df[numeric_cols].corr()
    
    print("\nğŸ“Š Korelasyon Matrisi:")
    print(correlation_matrix)
    
    # GÃ¼Ã§lÃ¼ korelasyonlarÄ± bul (|r| > 0.5)
    print("\nğŸ” GÃ¼Ã§lÃ¼ Korelasyonlar (|r| > 0.5):")
    strong_corr = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            if abs(correlation_matrix.iloc[i, j]) > 0.5:
                strong_corr.append({
                    'DeÄŸiÅŸken 1': correlation_matrix.columns[i],
                    'DeÄŸiÅŸken 2': correlation_matrix.columns[j],
                    'Korelasyon': correlation_matrix.iloc[i, j]
                })
    
    if strong_corr:
        strong_corr_df = pd.DataFrame(strong_corr).sort_values('Korelasyon', 
                                                                key=abs, 
                                                                ascending=False)
        print(strong_corr_df)
    else:
        print("   â€¢ GÃ¼Ã§lÃ¼ korelasyon bulunamadÄ±.")
    
    # Hedef deÄŸiÅŸken ile korelasyonlar
    if 'num' in numeric_cols:
        print("\nğŸ¯ Hedef DeÄŸiÅŸken (num) ile En YÃ¼ksek Korelasyonlar:")
        target_corr = correlation_matrix['num'].sort_values(ascending=False)
        print(target_corr)
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))
    
    # Tam korelasyon Ä±sÄ± haritasÄ±
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', 
                cmap='coolwarm', center=0, square=True, linewidths=1,
                cbar_kws={"shrink": 0.8}, ax=axes[0], vmin=-1, vmax=1)
    axes[0].set_title('Korelasyon Matrisi (Ãœst ÃœÃ§gen)', fontsize=14, fontweight='bold')
    
    # Hedef deÄŸiÅŸken korelasyonlarÄ± (bar plot)
    if 'num' in numeric_cols:
        target_corr_plot = correlation_matrix['num'].drop('num').sort_values()
        colors = ['red' if x < 0 else 'green' for x in target_corr_plot]
        target_corr_plot.plot(kind='barh', ax=axes[1], color=colors, alpha=0.7)
        axes[1].set_title('Hedef DeÄŸiÅŸken (num) ile Korelasyonlar', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Korelasyon KatsayÄ±sÄ±')
        axes[1].axvline(0, color='black', linewidth=0.8)
        axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('img/04_correlation_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return correlation_matrix, strong_corr_df if strong_corr else None

# ============================================================================
# 6. KATEGORÄ°K DEÄÄ°ÅKEN ANALÄ°ZLERÄ°
# ============================================================================

def categorical_analysis(df):
    """
    Kategorik deÄŸiÅŸkenlerin detaylÄ± analizi.
    """
    print("\n" + "="*80)
    print("6. KATEGORÄ°K DEÄÄ°ÅKEN ANALÄ°ZLERÄ°")
    print("="*80)
    
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    if 'id' in categorical_cols:
        categorical_cols.remove('id')
    if 'dataset' in categorical_cols:
        categorical_cols.remove('dataset')  # TÃ¼m veriler Cleveland'dan
    
    # Her kategorik deÄŸiÅŸken iÃ§in analiz
    for col in categorical_cols:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š {col.upper()} Analizi")
        print(f"{'='*80}")
        
        # Frekans tablosu
        freq_table = df[col].value_counts()
        freq_percent = df[col].value_counts(normalize=True) * 100
        
        freq_df = pd.DataFrame({
            'Frekans': freq_table,
            'YÃ¼zde (%)': freq_percent
        })
        print(freq_df)
        
        # Hedef deÄŸiÅŸken ile Ã§apraz tablo
        if 'num' in df.columns:
            print(f"\n{col} vs Kalp HastalÄ±ÄŸÄ± (num) - Ã‡apraz Tablo:")
            crosstab = pd.crosstab(df[col], df['num'], margins=True)
            print(crosstab)
            
            # Chi-square test
            chi2, p_value, dof, expected = chi2_contingency(pd.crosstab(df[col], df['num']))
            print(f"\nğŸ“ˆ Chi-Square Test SonuÃ§larÄ±:")
            print(f"   â€¢ Chi-square: {chi2:.4f}")
            print(f"   â€¢ p-deÄŸeri: {p_value:.6f}")
            print(f"   â€¢ Serbestlik derecesi: {dof}")
            
            if p_value < 0.05:
                print(f"   âœ“ {col} ve kalp hastalÄ±ÄŸÄ± arasÄ±nda istatistiksel olarak anlamlÄ± iliÅŸki var (p < 0.05)")
            else:
                print(f"   âœ— {col} ve kalp hastalÄ±ÄŸÄ± arasÄ±nda istatistiksel olarak anlamlÄ± iliÅŸki yok (p â‰¥ 0.05)")
    
    # GÃ¶rselleÅŸtirme
    n_cols_cat = len(categorical_cols)
    n_rows = (n_cols_cat + 1) // 2
    
    fig, axes = plt.subplots(n_rows, 2, figsize=(18, n_rows * 6))
    axes = axes.ravel() if n_cols_cat > 1 else [axes]
    
    for idx, col in enumerate(categorical_cols):
        if 'num' in df.columns:
            # Stacked bar chart
            crosstab_norm = pd.crosstab(df[col], df['num'], normalize='index') * 100
            crosstab_norm.plot(kind='bar', stacked=True, ax=axes[idx], 
                              colormap='viridis', alpha=0.8)
            axes[idx].set_title(f'{col} - Kalp HastalÄ±ÄŸÄ± DaÄŸÄ±lÄ±mÄ±', 
                               fontsize=12, fontweight='bold')
            axes[idx].set_xlabel(col)
            axes[idx].set_ylabel('YÃ¼zde (%)')
            axes[idx].legend(title='HastalÄ±k Derecesi', bbox_to_anchor=(1.05, 1))
            axes[idx].grid(True, alpha=0.3, axis='y')
            plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=45, ha='right')
        else:
            # Basit bar chart
            df[col].value_counts().plot(kind='bar', ax=axes[idx], color='steelblue', alpha=0.8)
            axes[idx].set_title(f'{col} DaÄŸÄ±lÄ±mÄ±', fontsize=12, fontweight='bold')
            axes[idx].set_xlabel(col)
            axes[idx].set_ylabel('Frekans')
            axes[idx].grid(True, alpha=0.3, axis='y')
            plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    # KullanÄ±lmayan subplot'larÄ± gizle
    for idx in range(n_cols_cat, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('img/05_categorical_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

# ============================================================================
# 7. Ä°STATÄ°STÄ°KSEL TESTLER
# ============================================================================

def statistical_tests(df):
    """
    Ã‡eÅŸitli istatistiksel testler uygular.
    """
    print("\n" + "="*80)
    print("7. Ä°STATÄ°STÄ°KSEL TESTLER")
    print("="*80)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'id' in numeric_cols:
        numeric_cols.remove('id')
    
    test_results = {}
    
    # Normallik testleri
    print("\nğŸ“Š NORMALLÄ°K TESTLERÄ°")
    print("-" * 80)
    print("Hâ‚€: Veriler normal daÄŸÄ±lÄ±ma uyar")
    print("Hâ‚: Veriler normal daÄŸÄ±lÄ±ma uymaz")
    print("Î± = 0.05")
    print("-" * 80)
    
    normality_results = []
    for col in numeric_cols:
        # Shapiro-Wilk testi
        shapiro_stat, shapiro_p = shapiro(df[col].dropna())
        
        # Kolmogorov-Smirnov testi
        ks_stat, ks_p = kstest(df[col].dropna(), 'norm')
        
        # Anderson-Darling testi (normaltest)
        anderson_stat, anderson_p = normaltest(df[col].dropna())
        
        normality_results.append({
            'DeÄŸiÅŸken': col,
            'Shapiro-Wilk p': shapiro_p,
            'K-S p': ks_p,
            'Normaltest p': anderson_p,
            'Normal?': 'Evet' if shapiro_p > 0.05 else 'HayÄ±r'
        })
        
        print(f"\n{col}:")
        print(f"   â€¢ Shapiro-Wilk: W={shapiro_stat:.4f}, p={shapiro_p:.6f}")
        print(f"   â€¢ K-S: D={ks_stat:.4f}, p={ks_p:.6f}")
        print(f"   â€¢ Normaltest: stat={anderson_stat:.4f}, p={anderson_p:.6f}")
        print(f"   â€¢ SonuÃ§: {'Normal daÄŸÄ±lÄ±m' if shapiro_p > 0.05 else 'Normal deÄŸil'}")
    
    normality_df = pd.DataFrame(normality_results)
    test_results['normality'] = normality_df
    
    # Cinsiyet ve yaÅŸ arasÄ±ndaki iliÅŸki (t-test)
    if 'sex' in df.columns and 'age' in numeric_cols:
        print("\nğŸ“Š CÄ°NSÄ°YET VE YAÅ Ä°LÄ°ÅKÄ°SÄ° (T-TEST)")
        print("-" * 80)
        
        male_ages = df[df['sex'] == 'Male']['age'].dropna()
        female_ages = df[df['sex'] == 'Female']['age'].dropna()
        
        t_stat, t_p = stats.ttest_ind(male_ages, female_ages)
        
        print(f"Hâ‚€: Erkek ve kadÄ±n yaÅŸ ortalamalarÄ± arasÄ±nda fark yoktur")
        print(f"Hâ‚: Erkek ve kadÄ±n yaÅŸ ortalamalarÄ± arasÄ±nda fark vardÄ±r")
        print(f"\nErkek yaÅŸ ortalamasÄ±: {male_ages.mean():.2f} Â± {male_ages.std():.2f}")
        print(f"KadÄ±n yaÅŸ ortalamasÄ±: {female_ages.mean():.2f} Â± {female_ages.std():.2f}")
        print(f"\nt-istatistik: {t_stat:.4f}")
        print(f"p-deÄŸeri: {t_p:.6f}")
        
        if t_p < 0.05:
            print(f"âœ“ AnlamlÄ± fark var (p < 0.05)")
        else:
            print(f"âœ— AnlamlÄ± fark yok (p â‰¥ 0.05)")
        
        test_results['t_test_sex_age'] = {'t_stat': t_stat, 'p_value': t_p}
    
    # ANOVA - GÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ± tÃ¼rÃ¼ ve yaÅŸ
    if 'cp' in df.columns and 'age' in numeric_cols:
        print("\nğŸ“Š GÃ–ÄÃœS AÄRISI TÃœRÃœ VE YAÅ Ä°LÄ°ÅKÄ°SÄ° (ANOVA)")
        print("-" * 80)
        
        groups = [group['age'].dropna() for name, group in df.groupby('cp')]
        f_stat, f_p = stats.f_oneway(*groups)
        
        print(f"Hâ‚€: TÃ¼m gÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ± tÃ¼rlerinin yaÅŸ ortalamalarÄ± eÅŸittir")
        print(f"Hâ‚: En az bir gÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ± tÃ¼rÃ¼nÃ¼n yaÅŸ ortalamasÄ± farklÄ±dÄ±r")
        print(f"\nF-istatistik: {f_stat:.4f}")
        print(f"p-deÄŸeri: {f_p:.6f}")
        
        if f_p < 0.05:
            print(f"âœ“ Gruplar arasÄ± anlamlÄ± fark var (p < 0.05)")
        else:
            print(f"âœ— Gruplar arasÄ± anlamlÄ± fark yok (p â‰¥ 0.05)")
        
        test_results['anova_cp_age'] = {'f_stat': f_stat, 'p_value': f_p}
    
    # Mann-Whitney U test (non-parametric)
    if 'sex' in df.columns and 'chol' in numeric_cols:
        print("\nğŸ“Š CÄ°NSÄ°YET VE KOLESTEROL Ä°LÄ°ÅKÄ°SÄ° (MANN-WHITNEY U)")
        print("-" * 80)
        
        male_chol = df[df['sex'] == 'Male']['chol'].dropna()
        female_chol = df[df['sex'] == 'Female']['chol'].dropna()
        
        u_stat, u_p = stats.mannwhitneyu(male_chol, female_chol)
        
        print(f"Hâ‚€: Erkek ve kadÄ±n kolesterol daÄŸÄ±lÄ±mlarÄ± aynÄ±dÄ±r")
        print(f"Hâ‚: Erkek ve kadÄ±n kolesterol daÄŸÄ±lÄ±mlarÄ± farklÄ±dÄ±r")
        print(f"\nErkek kolesterol: {male_chol.median():.2f} (medyan)")
        print(f"KadÄ±n kolesterol: {female_chol.median():.2f} (medyan)")
        print(f"\nU-istatistik: {u_stat:.4f}")
        print(f"p-deÄŸeri: {u_p:.6f}")
        
        if u_p < 0.05:
            print(f"âœ“ AnlamlÄ± fark var (p < 0.05)")
        else:
            print(f"âœ— AnlamlÄ± fark yok (p â‰¥ 0.05)")
        
        test_results['mannwhitney_sex_chol'] = {'u_stat': u_stat, 'p_value': u_p}
    
    return test_results

# ============================================================================
# 8. GELÄ°ÅMÄ°Å GÃ–RSELLEÅTÄ°RMELER
# ============================================================================

def advanced_visualizations(df):
    """
    KapsamlÄ± gÃ¶rselleÅŸtirmeler oluÅŸturur.
    """
    print("\n" + "="*80)
    print("8. GELÄ°ÅMÄ°Å GÃ–RSELLEÅTÄ°RMELER")
    print("="*80)
    
    # 1. Pair plot (sayÄ±sal deÄŸiÅŸkenler)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'id' in numeric_cols:
        numeric_cols.remove('id')
    
    # Ä°lk 5 en Ã¶nemli sayÄ±sal deÄŸiÅŸken iÃ§in pair plot
    important_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']
    important_cols = [col for col in important_cols if col in numeric_cols]
    
    if 'num' in df.columns:
        print("\nğŸ“Š Pair Plot oluÅŸturuluyor...")
        pairplot_df = df[important_cols + ['num']].copy()
        pairplot_df['num_binary'] = (pairplot_df['num'] > 0).astype(int)
        
        g = sns.pairplot(pairplot_df.drop('num', axis=1), hue='num_binary', 
                        palette={0: 'blue', 1: 'red'}, 
                        diag_kind='kde', plot_kws={'alpha': 0.6})
        g.fig.suptitle('Pair Plot - Ã–nemli DeÄŸiÅŸkenler (Mavi: SaÄŸlÄ±klÄ±, KÄ±rmÄ±zÄ±: Hasta)', 
                      y=1.02, fontsize=16, fontweight='bold')
        plt.savefig('img/06_pairplot_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    # 2. Violin plots
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    axes = axes.ravel()
    
    plot_cols = important_cols[:6]
    for idx, col in enumerate(plot_cols):
        if 'num' in df.columns:
            df_plot = df.copy()
            df_plot['num_binary'] = (df_plot['num'] > 0).astype(str)
            df_plot['num_binary'] = df_plot['num_binary'].map({'0': 'SaÄŸlÄ±klÄ±', '1': 'Hasta'})
            
            sns.violinplot(data=df_plot, x='num_binary', y=col, ax=axes[idx], 
                          palette={'SaÄŸlÄ±klÄ±': 'lightblue', 'Hasta': 'lightcoral'})
            axes[idx].set_title(f'{col} DaÄŸÄ±lÄ±mÄ± (Kalp HastalÄ±ÄŸÄ±na GÃ¶re)', 
                               fontsize=12, fontweight='bold')
            axes[idx].set_xlabel('Durum')
            axes[idx].set_ylabel(col)
            axes[idx].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('img/07_violin_plots.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 3. Kategorik deÄŸiÅŸkenler - count plots
    categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope']
    categorical_cols = [col for col in categorical_cols if col in df.columns]
    
    n_cat = len(categorical_cols)
    n_rows = (n_cat + 1) // 2
    
    fig, axes = plt.subplots(n_rows, 2, figsize=(18, n_rows * 5))
    axes = axes.ravel() if n_cat > 1 else [axes]
    
    for idx, col in enumerate(categorical_cols):
        if 'num' in df.columns:
            df_plot = df.copy()
            df_plot['num_binary'] = (df_plot['num'] > 0).astype(str)
            df_plot['num_binary'] = df_plot['num_binary'].map({'0': 'SaÄŸlÄ±klÄ±', '1': 'Hasta'})
            
            sns.countplot(data=df_plot, x=col, hue='num_binary', ax=axes[idx],
                         palette={'SaÄŸlÄ±klÄ±': 'steelblue', 'Hasta': 'orangered'})
            axes[idx].set_title(f'{col} - Kalp HastalÄ±ÄŸÄ± Durumu', 
                               fontsize=12, fontweight='bold')
            axes[idx].set_xlabel(col)
            axes[idx].set_ylabel('SayÄ±')
            axes[idx].legend(title='Durum')
            axes[idx].grid(True, alpha=0.3, axis='y')
            plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    # KullanÄ±lmayan subplot'larÄ± gizle
    for idx in range(n_cat, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('img/08_categorical_countplots.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 4. YaÅŸ daÄŸÄ±lÄ±mÄ± detaylÄ± analiz
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Histogram
    axes[0, 0].hist(df['age'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')
    axes[0, 0].set_title('YaÅŸ DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('YaÅŸ')
    axes[0, 0].set_ylabel('Frekans')
    axes[0, 0].axvline(df['age'].mean(), color='red', linestyle='--', 
                       linewidth=2, label=f"Ortalama: {df['age'].mean():.1f}")
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # KDE plot
    df['age'].plot(kind='kde', ax=axes[0, 1], color='darkblue', linewidth=2)
    axes[0, 1].set_title('YaÅŸ YoÄŸunluk DaÄŸÄ±lÄ±mÄ± (KDE)', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('YaÅŸ')
    axes[0, 1].set_ylabel('YoÄŸunluk')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Cinsiyet ve yaÅŸa gÃ¶re
    if 'sex' in df.columns:
        for sex in df['sex'].unique():
            df[df['sex'] == sex]['age'].plot(kind='kde', ax=axes[1, 0], 
                                              label=sex, linewidth=2, alpha=0.7)
        axes[1, 0].set_title('YaÅŸ DaÄŸÄ±lÄ±mÄ± (Cinsiyete GÃ¶re)', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('YaÅŸ')
        axes[1, 0].set_ylabel('YoÄŸunluk')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
    
    # YaÅŸ gruplarÄ±
    df['age_group'] = pd.cut(df['age'], bins=[0, 40, 50, 60, 100], 
                             labels=['<40', '40-50', '50-60', '60+'])
    age_group_counts = df['age_group'].value_counts().sort_index()
    age_group_counts.plot(kind='bar', ax=axes[1, 1], color='coral', alpha=0.8)
    axes[1, 1].set_title('YaÅŸ GruplarÄ± DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('YaÅŸ Grubu')
    axes[1, 1].set_ylabel('Frekans')
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=0)
    
    plt.tight_layout()
    plt.savefig('img/09_age_detailed_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

# ============================================================================
# 9. VERÄ° Ã–NÄ°ÅLEME Ã–NERÄ°LERÄ°
# ============================================================================

def preprocessing_recommendations(df, outlier_summary):
    """
    Veri Ã¶niÅŸleme iÃ§in Ã¶neriler sunar.
    """
    print("\n" + "="*80)
    print("9. VERÄ° Ã–NÄ°ÅLEME Ã–NERÄ°LERÄ° VE UYARILAR")
    print("="*80)
    
    print("\nâš ï¸  Ã–NEMLÄ° UYARILAR VE Ã–NERÄ°LER:")
    print("-" * 80)
    
    # 1. Eksik deÄŸer kontrolÃ¼
    missing_count = df.isnull().sum().sum()
    if missing_count > 0:
        print(f"\n1. EKSÄ°K DEÄERLER:")
        print(f"   â€¢ Toplam {missing_count} eksik deÄŸer tespit edildi.")
        print(f"   â€¢ Ã–NERÄ°: Eksik deÄŸerleri impute etmeden Ã¶nce:")
        print(f"     - Missing at Random (MAR) olup olmadÄ±ÄŸÄ±nÄ± kontrol edin")
        print(f"     - Medyan/mod yerine model-based imputation dÃ¼ÅŸÃ¼nÃ¼n")
        print(f"     - Eksik deÄŸerlerin pattern'ini inceleyin")
    else:
        print(f"\n1. EKSÄ°K DEÄERLER:")
        print(f"   âœ“ Veri setinde eksik deÄŸer yok - harika!")
    
    # 2. Outlier analizi
    print(f"\n2. OUTLIER (AYKIRI DEÄER) YÃ–NETÄ°MÄ°:")
    print(f"   âš ï¸  UYARI: Outlier'larÄ± otomatik olarak silmeyin!")
    print(f"   â€¢ Medikal verilerde outlier'lar Ã¶nemli bilgiler iÃ§erebilir")
    print(f"   â€¢ Ã–NERÄ°LER:")
    print(f"     - Outlier'larÄ±n klinik olarak anlamlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol edin")
    print(f"     - Veri giriÅŸ hatalarÄ± iÃ§in manuel kontrol yapÄ±n")
    print(f"     - Robust scaler (IQR-based) kullanmayÄ± dÃ¼ÅŸÃ¼nÃ¼n")
    print(f"     - Winsorization (kÄ±rpma) ile extreme deÄŸerleri sÄ±nÄ±rlayÄ±n")
    
    high_outlier_cols = [col for col, info in outlier_summary.items() 
                         if info['percent'] > 5]
    if high_outlier_cols:
        print(f"\n   YÃ¼ksek outlier oranÄ±na sahip deÄŸiÅŸkenler (>%5):")
        for col in high_outlier_cols:
            print(f"     - {col}: %{outlier_summary[col]['percent']:.1f}")
    
    # 3. Normalizasyon uyarÄ±larÄ±
    print(f"\n3. NORMALÄ°ZASYON/Ã–LÃ‡EKLENDÄ°RME:")
    print(f"   âš ï¸  UYARI: TÃ¼m deÄŸiÅŸkenlere aynÄ± Ã¶lÃ§eklendirmeyi uygulamayÄ±n!")
    print(f"   â€¢ Ã–NERÄ°LER:")
    print(f"     - Tree-based modeller (Random Forest, XGBoost) iÃ§in Ã¶lÃ§eklendirme GEREKMÄ°YOR")
    print(f"     - Logistic Regression, SVM, Neural Networks iÃ§in StandardScaler kullanÄ±n")
    print(f"     - MinMaxScaler outlier'lara duyarlÄ±dÄ±r, dikkatli kullanÄ±n")
    print(f"     - ID sÃ¼tununu Ã¶lÃ§eklendirmeyin ve model eÄŸitiminde kullanmayÄ±n")
    
    # 4. Kategorik kodlama
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    if 'id' in categorical_cols:
        categorical_cols.remove('id')
    
    print(f"\n4. KATEGORÄ°K DEÄÄ°ÅKEN KODLAMA:")
    print(f"   â€¢ {len(categorical_cols)} kategorik deÄŸiÅŸken var")
    print(f"   â€¢ Ã–NERÄ°LER:")
    print(f"     - Binary deÄŸiÅŸkenler (sex, fbs, exang): Label Encoding (0/1)")
    print(f"     - Ordinal deÄŸiÅŸkenler (cp, slope): Ordinal Encoding")
    print(f"     - Nominal deÄŸiÅŸkenler: One-Hot Encoding")
    print(f"     - YÃ¼ksek kardinalite iÃ§in Target Encoding dÃ¼ÅŸÃ¼nÃ¼n")
    print(f"     - Tree-based modeller iÃ§in Label Encoding yeterli")
    
    # 5. Hedef deÄŸiÅŸken dengesizliÄŸi
    if 'num' in df.columns:
        target_dist = df['num'].value_counts()
        target_percent = df['num'].value_counts(normalize=True) * 100
        
        print(f"\n5. HEDEF DEÄÄ°ÅKEN DENGESÄ°:")
        print(f"   â€¢ SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
        for cls, count in target_dist.items():
            print(f"     - SÄ±nÄ±f {cls}: {count} ({target_percent[cls]:.1f}%)")
        
        # Dengesizlik kontrolÃ¼
        max_class_ratio = target_percent.max() / target_percent.min()
        if max_class_ratio > 2:
            print(f"\n   âš ï¸  UYARI: SÄ±nÄ±f dengesizliÄŸi tespit edildi (oran: {max_class_ratio:.1f})")
            print(f"   â€¢ Ã–NERÄ°LER:")
            print(f"     - SMOTE (Synthetic Minority Over-sampling) kullanÄ±n")
            print(f"     - Class weights ayarlayÄ±n")
            print(f"     - Stratified sampling kullanÄ±n (train/test split)")
            print(f"     - Under-sampling yerine over-sampling tercih edin")
        else:
            print(f"\n   âœ“ SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± makul seviyede dengeli")
    
    # 6. Feature engineering Ã¶nerileri
    print(f"\n6. Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ° Ã–NERÄ°LERÄ°:")
    print(f"   â€¢ YaÅŸ gruplarÄ± oluÅŸturabilirsiniz (30-40, 40-50, etc.)")
    print(f"   â€¢ BMI hesaplayabilirsiniz (eÄŸer kilo/boy varsa)")
    print(f"   â€¢ Risk skorlarÄ± oluÅŸturabilirsiniz (Ã§oklu risk faktÃ¶rÃ¼ kombinasyonu)")
    print(f"   â€¢ EtkileÅŸim terimleri ekleyin (age*sex, chol*age, etc.)")
    print(f"   â€¢ Polinomial Ã¶zellikler (ageÂ², cholÂ², etc.) deneyebilirsiniz")
    
    # 7. Veri bÃ¶lme stratejisi
    print(f"\n7. VERÄ° BÃ–LME STRATEJÄ°SÄ°:")
    print(f"   â€¢ Ã–NERÄ°LER:")
    print(f"     - Train/Test: 80/20 veya 70/30 oranÄ± kullanÄ±n")
    print(f"     - MUTLAKA Stratified Split kullanÄ±n (hedef deÄŸiÅŸken dengesini koru)")
    print(f"     - Cross-validation iÃ§in K-Fold (k=5 veya k=10)")
    print(f"     - KÃ¼Ã§Ã¼k veri seti iÃ§in Leave-One-Out CV dÃ¼ÅŸÃ¼nÃ¼n")
    print(f"     - Random state sabitleyerek reproducibility saÄŸlayÄ±n")
    
    # 8. Ã‡oklu doÄŸrusallÄ±k kontrolÃ¼
    print(f"\n8. Ã‡OKLU DOÄRUSALLIK (MULTICOLLINEARITY):")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'id' in numeric_cols:
        numeric_cols.remove('id')
    
    correlation_matrix = df[numeric_cols].corr()
    high_corr_pairs = []
    
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            if abs(correlation_matrix.iloc[i, j]) > 0.8:
                high_corr_pairs.append({
                    'Var1': correlation_matrix.columns[i],
                    'Var2': correlation_matrix.columns[j],
                    'Corr': correlation_matrix.iloc[i, j]
                })
    
    if high_corr_pairs:
        print(f"   âš ï¸  UYARI: YÃ¼ksek korelasyonlu deÄŸiÅŸken Ã§iftleri bulundu (|r| > 0.8):")
        for pair in high_corr_pairs:
            print(f"     - {pair['Var1']} vs {pair['Var2']}: r={pair['Corr']:.3f}")
        print(f"\n   â€¢ Ã–NERÄ°LER:")
        print(f"     - VIF (Variance Inflation Factor) hesaplayÄ±n")
        print(f"     - YÃ¼ksek korelasyonlu deÄŸiÅŸkenlerden birini Ã§Ä±karÄ±n")
        print(f"     - PCA ile boyut indirgeme yapÄ±n")
        print(f"     - Ridge/Lasso regression kullanÄ±n (regularization)")
    else:
        print(f"   âœ“ Ciddi Ã§oklu doÄŸrusallÄ±k problemi yok (|r| < 0.8)")
    
    # 9. Model seÃ§imi Ã¶nerileri
    print(f"\n9. MODEL SEÃ‡Ä°MÄ° Ã–NERÄ°LERÄ°:")
    print(f"   â€¢ BaÅŸlangÄ±Ã§ iÃ§in Ã¶nerilen modeller:")
    print(f"     1. Logistic Regression (baseline)")
    print(f"     2. Random Forest (feature importance iÃ§in)")
    print(f"     3. XGBoost (yÃ¼ksek performans iÃ§in)")
    print(f"     4. SVM (non-linear relationships iÃ§in)")
    print(f"   â€¢ Model karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in:")
    print(f"     - Accuracy, Precision, Recall, F1-Score")
    print(f"     - ROC-AUC (class imbalance varsa)")
    print(f"     - Confusion Matrix analizi")
    
    # 10. Validasyon stratejisi
    print(f"\n10. VALIDASYON STRATEJÄ°SÄ°:")
    print(f"    â€¢ Overfitting kontrolÃ¼ iÃ§in:")
    print(f"      - Learning curves Ã§izin")
    print(f"      - Train vs Test performance karÅŸÄ±laÅŸtÄ±rÄ±n")
    print(f"      - Cross-validation kullanÄ±n")
    print(f"      - Regularization uygulayÄ±n (L1/L2)")
    print(f"    â€¢ Underfitting kontrolÃ¼ iÃ§in:")
    print(f"      - Model complexity artÄ±rÄ±n")
    print(f"      - Feature engineering yapÄ±n")
    print(f"      - Ensemble methods deneyin")
    
    print("\n" + "="*80)
    print("âœ… VERÄ° Ã–NÄ°ÅLEME Ã–NERÄ°LERÄ° TAMAMLANDI")
    print("="*80)

# ============================================================================
# ANA ANALÄ°Z FONKSIYONU
# ============================================================================

def main():
    """
    TÃ¼m analizleri sÄ±rasÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±r.
    """
    # Veri dosya yolu
    data_path = 'data/heart_disease_uci.csv'
    
    try:
        # 1. Veri yÃ¼kleme
        df = load_and_explore_data(data_path)
        
        # 2. Veri kalitesi analizi
        missing_df = analyze_data_quality(df)
        
        # 3. TanÄ±mlayÄ±cÄ± istatistikler
        stats_df, additional_stats = descriptive_statistics(df)
        
        # 4. Outlier analizi
        outlier_summary, z_outlier_summary = detect_outliers(df)
        
        # 5. Korelasyon analizi
        correlation_matrix, strong_corr_df = correlation_analysis(df)
        
        # 6. Kategorik deÄŸiÅŸken analizi
        categorical_analysis(df)
        
        # 7. Ä°statistiksel testler
        test_results = statistical_tests(df)
        
        # 8. GeliÅŸmiÅŸ gÃ¶rselleÅŸtirmeler
        advanced_visualizations(df)
        
        # 9. Veri Ã¶niÅŸleme Ã¶nerileri
        preprocessing_recommendations(df, outlier_summary)
        
        print("\n" + "="*80)
        print("ğŸ‰ ANALÄ°Z TAMAMLANDI!")
        print("="*80)
        print("\nğŸ“ OluÅŸturulan GÃ¶rsel Dosyalar:")
        print("   1. 01_data_quality_analysis.png")
        print("   2. 02_descriptive_statistics_distributions.png")
        print("   3. 03_outlier_analysis_boxplots.png")
        print("   4. 04_correlation_analysis.png")
        print("   5. 05_categorical_analysis.png")
        print("   6. 06_pairplot_analysis.png")
        print("   7. 07_violin_plots.png")
        print("   8. 08_categorical_countplots.png")
        print("   9. 09_age_detailed_analysis.png")
        
        print("\nğŸ’¾ Rapor iÃ§in 'HEART_DISEASE_ANALYSIS_TECHNICAL_REPORT.md' dosyasÄ± oluÅŸturulacak...")
        
        # SonuÃ§larÄ± return et (rapor iÃ§in kullanÄ±lacak)
        return {
            'df': df,
            'missing_df': missing_df,
            'stats_df': stats_df,
            'additional_stats': additional_stats,
            'outlier_summary': outlier_summary,
            'z_outlier_summary': z_outlier_summary,
            'correlation_matrix': correlation_matrix,
            'strong_corr_df': strong_corr_df,
            'test_results': test_results
        }
        
    except FileNotFoundError:
        print(f"âŒ HATA: '{data_path}' dosyasÄ± bulunamadÄ±!")
        print("LÃ¼tfen dosya yolunu kontrol edin.")
        return None
    except Exception as e:
        print(f"âŒ HATA: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# SCRIPT Ã‡ALIÅTIRMA
# ============================================================================

if __name__ == "__main__":
    results = main()
