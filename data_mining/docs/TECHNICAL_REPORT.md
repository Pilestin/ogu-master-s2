# UCI Heart Disease Veri Seti Ãœzerinde Makine Ã–ÄŸrenmesi ile Kalp HastalÄ±ÄŸÄ± Tahmini

## Teknik Rapor ve DokÃ¼mantasyon

**Tarih:** 1 Ocak 2025  
**Veri Seti:** UCI Heart Disease (Cleveland)  
**Analiz TÃ¼rÃ¼:** SÄ±nÄ±flandÄ±rma (Binary Classification)

---

## 1. GiriÅŸ ve Problem TanÄ±mÄ±

### 1.1 AmaÃ§

Bu Ã§alÄ±ÅŸmada, UCI Heart Disease veri seti kullanÄ±larak kalp hastalÄ±ÄŸÄ± tahmin modelleri geliÅŸtirilmiÅŸtir. Ã‡alÄ±ÅŸmanÄ±n temel hedefleri:

1. FarklÄ± veri Ã¶niÅŸleme tekniklerinin model performansÄ±na etkisini izole olarak incelemek (Ablation Study)
2. Hiperparametre optimizasyonu (Optuna) ile model performansÄ±nÄ± maksimize etmek
3. 6 farklÄ± makine Ã¶ÄŸrenmesi algoritmasÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmak
4. En iyi ve en kÃ¶tÃ¼ modellerin tÃ¼m tekniklerle birlikte nasÄ±l performans gÃ¶sterdiÄŸini analiz etmek

### 1.2 Veri Seti Ã–zellikleri

- **Kaynak:** UCI Machine Learning Repository
- **Alt Veri Seti:** Cleveland (en yaygÄ±n kullanÄ±lan)
- **Ã–rneklem SayÄ±sÄ±:** 304
- **Ã–zellik SayÄ±sÄ±:** 13 (orijinal) + 4 (mÃ¼hendislik) = 17
- **Hedef DeÄŸiÅŸken:** Binary (0: SaÄŸlÄ±klÄ±, 1: Kalp HastalÄ±ÄŸÄ±)
- **SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:** SaÄŸlÄ±klÄ±: 165 (%54.3), Hasta: 139 (%45.7)

### 1.3 Deneysel TasarÄ±m (Ablation Study)

Bu Ã§alÄ±ÅŸmada her tekniÄŸin etkisini izole olarak gÃ¶rmek iÃ§in 6 farklÄ± senaryo tasarlanmÄ±ÅŸtÄ±r:

| Senaryo | Ä°Ã§erik | Scaler | FE | PCA | SMOTE | Optuna | CV |
|---------|--------|--------|-----|-----|-------|--------|-----|
| **S0: Baseline** | Temel | RobustScaler | âŒ | âŒ | âŒ | âŒ | 10-Fold |
| **S1: + PCA** | PCA etkisi | StandardScaler | âŒ | âœ… | âŒ | âŒ | 10-Fold |
| **S2: + FE** | Feature Eng. etkisi | RobustScaler | âœ… | âŒ | âŒ | âŒ | 10-Fold |
| **S3: + SMOTE** | Dengeleme etkisi | RobustScaler | âŒ | âŒ | âœ… | âŒ | 10-Fold |
| **S4: + Optuna** | HP optimizasyonu | RobustScaler | âŒ | âŒ | âŒ | âœ… | 10-Fold |
| **S5: All Combined** | TÃ¼m teknikler | StandardScaler | âœ… | âœ… | âœ… | âœ… | 10-Fold |

---

## 2. Veri Seti Ã–zellikleri (Features)

### 2.1 Orijinal Ã–zellikler

| Ã–zellik    | AÃ§Ä±klama                              | Tip       | DeÄŸer AralÄ±ÄŸÄ±                                              |
| ---------- | ------------------------------------- | --------- | ---------------------------------------------------------- |
| `age`      | YaÅŸ                                   | SÃ¼rekli   | 28-77 yÄ±l                                                  |
| `sex`      | Cinsiyet                              | Kategorik | Male, Female                                               |
| `cp`       | GÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ± tipi                     | Kategorik | typical angina, atypical angina, non-anginal, asymptomatic |
| `trestbps` | Dinlenme kan basÄ±ncÄ±                  | SÃ¼rekli   | 94-200 mmHg                                                |
| `chol`     | Serum kolesterol                      | SÃ¼rekli   | 126-564 mg/dl                                              |
| `fbs`      | AÃ§lÄ±k kan ÅŸekeri > 120 mg/dl          | Binary    | TRUE, FALSE                                                |
| `restecg`  | Dinlenme EKG sonucu                   | Kategorik | normal, st-t abnormality, lv hypertrophy                   |
| `thalch`   | Maksimum kalp hÄ±zÄ±                    | SÃ¼rekli   | 71-202 bpm                                                 |
| `exang`    | Egzersize baÄŸlÄ± angina                | Binary    | TRUE, FALSE                                                |
| `oldpeak`  | Egzersizin neden olduÄŸu ST depresyonu | SÃ¼rekli   | -2.6 - 6.2                                                 |
| `slope`    | ST segment eÄŸimi                      | Kategorik | upsloping, flat, downsloping                               |
| `ca`       | Floroskopi ile boyanan damar sayÄ±sÄ±   | Ordinal   | 0-3                                                        |
| `thal`     | Talasemi                              | Kategorik | normal, fixed defect, reversable defect                    |

### 2.2 MÃ¼hendislik Ã–zellikleri (Feature Engineering)

| Yeni Ã–zellik          | FormÃ¼l                            | GerekÃ§e                                                        |
| --------------------- | --------------------------------- | -------------------------------------------------------------- |
| `risk_score`          | (age Ã— chol) / 10000              | YaÅŸ ve kolesterol etkileÅŸimi - kardiyovaskÃ¼ler risk gÃ¶stergesi |
| `age_group`           | Binning (0-40, 40-55, 55-70, 70+) | YaÅŸ kategorileri ile risk gruplandÄ±rmasÄ±                       |
| `hr_age_ratio`        | thalch / (age + 1)                | YaÅŸa gÃ¶re normalize kalp hÄ±zÄ± performansÄ±                      |
| `bp_chol_interaction` | (trestbps Ã— chol) / 10000         | Kan basÄ±ncÄ± ve kolesterol etkileÅŸimi                           |

---

## 3. Metodoloji

### 3.1 Veri Ã–niÅŸleme Pipeline

```
Ham Veri (920 satÄ±r)
    â”‚
    â”œâ”€â”€ 1. Cleveland Filtresi â†’ 304 satÄ±r
    â”‚
    â”œâ”€â”€ 2. Kategorik Encoding (LabelEncoder)
    â”‚     â”œâ”€â”€ sex: 2 kategori â†’ [0, 1]
    â”‚     â”œâ”€â”€ cp: 4 kategori â†’ [0, 1, 2, 3]
    â”‚     â”œâ”€â”€ restecg: 3 kategori â†’ [0, 1, 2]
    â”‚     â”œâ”€â”€ exang: 2 kategori â†’ [0, 1]
    â”‚     â”œâ”€â”€ slope: 3 kategori â†’ [0, 1, 2]
    â”‚     â”œâ”€â”€ thal: 4 kategori â†’ [0, 1, 2, 3]
    â”‚     â””â”€â”€ fbs: 2 kategori â†’ [0, 1]
    â”‚
    â”œâ”€â”€ 3. Eksik DeÄŸer Doldurma (KNN Imputer, k=5)
    â”‚
    â”œâ”€â”€ 4. Ã–lÃ§ekleme (Senaryoya gÃ¶re)
    â”‚     â”œâ”€â”€ RobustScaler: S0, S2, S3, S4
    â”‚     â””â”€â”€ StandardScaler: S1, S5 (PCA iÃ§in zorunlu)
    â”‚
    â””â”€â”€ 5. Senaryoya BaÄŸlÄ± Ek Ä°ÅŸlemler
          â”œâ”€â”€ S1: PCA (%95 varyans)
          â”œâ”€â”€ S2: Feature Engineering (+4 Ã¶zellik)
          â”œâ”€â”€ S3: SMOTE (sÄ±nÄ±f dengeleme)
          â”œâ”€â”€ S4: Optuna (hiperparametre optimizasyonu)
          â””â”€â”€ S5: FE + PCA + SMOTE + Optuna
```

### 3.2 KullanÄ±lan Teknikler

#### 3.2.1 KNN Imputer

```python
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df_processed[numeric_cols] = imputer.fit_transform(df_processed[numeric_cols])
```

- **Parametre:** k=5 (en yakÄ±n 5 komÅŸu)
- **Avantaj:** Benzer Ã¶rneklerin deÄŸerlerini kullanÄ±r

#### 3.2.2 RobustScaler vs StandardScaler

| Scaler | FormÃ¼l | KullanÄ±m Senaryosu |
|--------|--------|-------------------|
| **RobustScaler** | `(X - median) / IQR` | S0, S2, S3, S4 - AykÄ±rÄ± deÄŸerlere dayanÄ±klÄ± |
| **StandardScaler** | `(X - mean) / std` | S1, S5 - PCA iÃ§in zorunlu |

#### 3.2.3 SMOTE

```python
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)
```

- **SonuÃ§:** 165 SaÄŸlÄ±klÄ± vs 139 Hasta â†’ 165 SaÄŸlÄ±klÄ± vs 165 Hasta

#### 3.2.4 PCA

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95, random_state=42)
X_pca = pca.fit_transform(X_scaled)
```

- **SonuÃ§:** 13 Ã¶zellik â†’ 12 bileÅŸen (%97.14 varyans korundu)

### 3.3 Validasyon Stratejisi

TÃ¼m senaryolarda **Stratified 10-Fold Cross Validation** kullanÄ±lmÄ±ÅŸtÄ±r:

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
```

**Neden 10-Fold CV?**
1. Her fold'da sÄ±nÄ±f oranlarÄ± korunur
2. 10 farklÄ± test seti ile gÃ¼venilir performans tahmini
3. Standart sapma model kararlÄ±lÄ±ÄŸÄ±nÄ± gÃ¶sterir

### 3.4 Hiperparametre Optimizasyonu (Optuna)

```python
import optuna

def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 50, 300)
    max_depth = trial.suggest_int('max_depth', 3, 20)
    # ...
    model = RandomForestClassifier(...)
    return cross_val_score(model, X, y, cv=skf, scoring='f1').mean()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30, show_progress_bar=True)
```

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Algoritma** | TPE (Bayesian Optimizasyon) |
| **Trial SayÄ±sÄ±** | 30 (S4), 50 (S5) |
| **Metrik** | F1-Score (maximize) |

---

## 4. KullanÄ±lan Modeller

### 4.1 Modeller ve VarsayÄ±lan Parametreleri

| Model | VarsayÄ±lan Parametreler |
|-------|------------------------|
| **Logistic Regression** | `max_iter=1000, random_state=42` |
| **Random Forest** | `random_state=42, n_jobs=-1` |
| **SVM** | `probability=True, random_state=42` |
| **Naive Bayes** | `GaussianNB()` (varsayÄ±lan) |
| **XGBoost** | `random_state=42, n_jobs=-1, eval_metric='logloss'` |
| **KNN** | `n_jobs=-1` |

### 4.2 Optuna Hiperparametre Arama UzaylarÄ±

#### Logistic Regression
| Parametre | AralÄ±k |
|-----------|--------|
| `C` | [0.01, 10.0] (log scale) |
| `penalty` | ['l1', 'l2'] |

#### Random Forest
| Parametre | AralÄ±k |
|-----------|--------|
| `n_estimators` | [50, 300] |
| `max_depth` | [3, 20] |
| `min_samples_split` | [2, 20] |
| `min_samples_leaf` | [1, 10] |

#### SVM
| Parametre | AralÄ±k |
|-----------|--------|
| `C` | [0.1, 100.0] (log scale) |
| `gamma` | ['scale', 'auto'] |
| `kernel` | ['rbf', 'poly'] |

#### Naive Bayes
| Parametre | AralÄ±k |
|-----------|--------|
| `var_smoothing` | [1e-12, 1e-6] (log scale) |

#### XGBoost
| Parametre | AralÄ±k |
|-----------|--------|
| `n_estimators` | [50, 300] |
| `max_depth` | [3, 15] |
| `learning_rate` | [0.01, 0.3] (log scale) |
| `subsample` | [0.6, 1.0] |
| `colsample_bytree` | [0.6, 1.0] |

#### KNN
| Parametre | AralÄ±k |
|-----------|--------|
| `n_neighbors` | [3, 21] (tek sayÄ±lar) |
| `weights` | ['uniform', 'distance'] |
| `metric` | ['euclidean', 'manhattan'] |

---

## 5. Deneysel SonuÃ§lar

### 5.1 Senaryo 0: Baseline

**KonfigÃ¼rasyon:** RobustScaler + 10-Fold CV + 6 Model (varsayÄ±lan parametreler)

| Model | Accuracy | Recall | F1-Score | AUC |
|-------|----------|--------|----------|-----|
| **Logistic Regression** ğŸ† | 0.842Â±0.053 | 0.771Â±0.076 | **0.817Â±0.061** | 0.911Â±0.056 |
| SVM | 0.832Â±0.055 | 0.771Â±0.099 | 0.806Â±0.069 | 0.900Â±0.056 |
| KNN | 0.822Â±0.082 | 0.771Â±0.126 | 0.796Â±0.102 | 0.877Â±0.066 |
| Naive Bayes | 0.819Â±0.049 | 0.785Â±0.084 | 0.798Â±0.058 | 0.895Â±0.055 |
| Random Forest | 0.809Â±0.046 | 0.757Â±0.096 | 0.783Â±0.052 | 0.896Â±0.045 |
| **XGBoost** ğŸ“‰ | 0.763Â±0.066 | 0.721Â±0.133 | **0.732Â±0.088** | 0.881Â±0.043 |

**Bulgular:**
- ğŸ† **En Ä°yi:** Logistic Regression (F1=0.817)
- ğŸ“‰ **En KÃ¶tÃ¼:** XGBoost (F1=0.732)
- Senaryo 5 iÃ§in bu iki model seÃ§ildi

### 5.2 Senaryo 1: + PCA

**KonfigÃ¼rasyon:** StandardScaler + PCA(%95) + 10-Fold CV + 6 Model  
**PCA Sonucu:** 13 Ã¶zellik â†’ 12 bileÅŸen (%97.14 varyans)

| Model | Accuracy | Recall | F1-Score | AUC |
|-------|----------|--------|----------|-----|
| **Logistic Regression** ğŸ† | 0.845Â±0.049 | 0.771Â±0.069 | **0.820Â±0.056** | 0.910Â±0.060 |
| XGBoost | 0.822Â±0.051 | 0.799Â±0.099 | 0.803Â±0.061 | 0.887Â±0.062 |
| SVM | 0.825Â±0.058 | 0.756Â±0.120 | 0.794Â±0.080 | 0.900Â±0.050 |
| Random Forest | 0.802Â±0.063 | 0.771Â±0.119 | 0.780Â±0.072 | 0.878Â±0.052 |
| KNN | 0.799Â±0.048 | 0.757Â±0.096 | 0.774Â±0.055 | 0.873Â±0.053 |
| Naive Bayes | 0.806Â±0.075 | 0.735Â±0.114 | 0.774Â±0.090 | 0.893Â±0.061 |

**Baseline'a GÃ¶re DeÄŸiÅŸim:**
- LR: +0.3% F1 iyileÅŸme
- XGBoost: +7.1% F1 iyileÅŸme (en Ã§ok fayda gÃ¶ren)

### 5.3 Senaryo 2: + Feature Engineering

**KonfigÃ¼rasyon:** RobustScaler + 4 Yeni Ã–zellik + 10-Fold CV + 6 Model  
**Yeni Ã–zellikler:** risk_score, age_group, hr_age_ratio, bp_chol_interaction

| Model | Accuracy | Recall | F1-Score | AUC |
|-------|----------|--------|----------|-----|
| **Logistic Regression** ğŸ† | 0.839Â±0.060 | 0.778Â±0.080 | **0.815Â±0.068** | 0.910Â±0.059 |
| Naive Bayes | 0.812Â±0.047 | 0.785Â±0.070 | 0.793Â±0.050 | 0.878Â±0.069 |
| Random Forest | 0.812Â±0.077 | 0.757Â±0.115 | 0.786Â±0.089 | 0.895Â±0.064 |
| SVM | 0.819Â±0.065 | 0.721Â±0.120 | 0.781Â±0.086 | 0.898Â±0.061 |
| XGBoost | 0.789Â±0.062 | 0.771Â±0.114 | 0.769Â±0.071 | 0.887Â±0.037 |
| KNN | 0.799Â±0.098 | 0.735Â±0.127 | 0.769Â±0.114 | 0.870Â±0.068 |

**Baseline'a GÃ¶re DeÄŸiÅŸim:**
- LR: -0.2% F1 (minimal etki)
- Feature engineering bu veri setinde etkisiz

### 5.4 Senaryo 3: + SMOTE

**KonfigÃ¼rasyon:** RobustScaler + SMOTE + 10-Fold CV + 6 Model  
**SMOTE Sonucu:** 165 vs 139 â†’ 165 vs 165 (dengeli sÄ±nÄ±flar)

| Model | Accuracy | Recall | F1-Score | AUC |
|-------|----------|--------|----------|-----|
| **Logistic Regression** ğŸ† | 0.842Â±0.074 | 0.806Â±0.066 | **0.837Â±0.075** | 0.908Â±0.055 |
| KNN | 0.830Â±0.067 | 0.824Â±0.104 | 0.827Â±0.074 | 0.901Â±0.067 |
| XGBoost | 0.830Â±0.068 | 0.811Â±0.081 | 0.826Â±0.073 | 0.899Â±0.055 |
| SVM | 0.836Â±0.056 | 0.799Â±0.091 | 0.828Â±0.063 | 0.902Â±0.053 |
| Random Forest | 0.830Â±0.076 | 0.805Â±0.096 | 0.824Â±0.083 | 0.911Â±0.060 |
| Naive Bayes | 0.821Â±0.060 | 0.781Â±0.094 | 0.811Â±0.070 | 0.889Â±0.047 |

**Baseline'a GÃ¶re DeÄŸiÅŸim:**
- LR: +2.0% F1 iyileÅŸme
- XGBoost: +9.4% F1 iyileÅŸme (en Ã§ok fayda gÃ¶ren!)
- **SMOTE tÃ¼m modellerde Ã¶nemli iyileÅŸme saÄŸladÄ±**

### 5.5 Senaryo 4: + Optuna

**KonfigÃ¼rasyon:** RobustScaler + Optuna(30 trial) + 10-Fold CV + 6 Model

| Model | Accuracy | Recall | F1-Score | AUC |
|-------|----------|--------|----------|-----|
| **Random Forest** ğŸ† | 0.848Â±0.056 | 0.778Â±0.086 | **0.824Â±0.065** | 0.914Â±0.048 |
| XGBoost | 0.836Â±0.049 | 0.814Â±0.065 | 0.820Â±0.049 | 0.906Â±0.045 |
| Logistic Regression | 0.842Â±0.053 | 0.771Â±0.076 | 0.817Â±0.061 | 0.911Â±0.057 |
| SVM | 0.845Â±0.072 | 0.757Â±0.111 | 0.815Â±0.090 | 0.906Â±0.055 |
| KNN | 0.829Â±0.056 | 0.764Â±0.090 | 0.802Â±0.068 | 0.906Â±0.044 |
| Naive Bayes | 0.819Â±0.049 | 0.785Â±0.084 | 0.798Â±0.058 | 0.895Â±0.055 |

**Baseline'a GÃ¶re DeÄŸiÅŸim:**
- RF: +4.1% F1 iyileÅŸme (Optuna'dan en Ã§ok fayda gÃ¶ren)
- XGBoost: +8.8% F1 iyileÅŸme
- **Optuna tÃ¼m modellerde iyileÅŸtirme saÄŸladÄ±**

### 5.6 Senaryo 5: All Combined

**KonfigÃ¼rasyon:** StandardScaler + FE + PCA + SMOTE + Optuna(50 trial) + 10-Fold CV  
**Test Edilen Modeller:** En iyi (LR) ve en kÃ¶tÃ¼ (XGBoost) modeller  
**Pipeline:** 17 Ã¶zellik â†’ PCA: 12 â†’ SMOTE: 330 Ã¶rnek

| Model | Accuracy | Recall | F1-Score | AUC |
|-------|----------|--------|----------|-----|
| **Logistic Regression (Best)** ğŸ† | 0.845Â±0.064 | 0.824Â±0.051 | **0.843Â±0.064** | 0.916Â±0.048 |
| **XGBoost (Worst)** | 0.830Â±0.053 | 0.849Â±0.049 | **0.834Â±0.051** | 0.909Â±0.038 |

**Baseline'a GÃ¶re DeÄŸiÅŸim:**
- LR: +2.6% F1 iyileÅŸme (0.817 â†’ 0.843)
- XGBoost: **+10.2% F1 iyileÅŸme** (0.732 â†’ 0.834) ğŸš€

---

## 6. Senaryo KarÅŸÄ±laÅŸtÄ±rmasÄ± Ã–zeti

### 6.1 TÃ¼m SenaryolarÄ±n Ã–zeti

| Senaryo | Ortalama F1 | En Ä°yi F1 | En Ä°yi Model |
|---------|-------------|-----------|--------------|
| **S0: Baseline** | 0.788 | 0.817 | Logistic Regression |
| **S1: + PCA** | 0.791 | 0.820 | Logistic Regression |
| **S2: + FE** | 0.785 | 0.815 | Logistic Regression |
| **S3: + SMOTE** | 0.826 | 0.837 | Logistic Regression |
| **S4: + Optuna** | 0.813 | 0.824 | Random Forest |
| **S5: All Combined** ğŸ† | **0.838** | **0.843** | Logistic Regression |

### 6.2 Teknik BazÄ±nda Etki Analizi

| Teknik | Ortalama F1 ArtÄ±ÅŸÄ± | En Ã‡ok Fayda GÃ¶ren Model |
|--------|-------------------|--------------------------|
| **PCA** | +0.3% | XGBoost (+7.1%) |
| **Feature Engineering** | -0.3% | - (Etkisiz) |
| **SMOTE** | +3.8% | XGBoost (+9.4%) |
| **Optuna** | +2.5% | RF (+4.1%), XGBoost (+8.8%) |
| **All Combined** | +5.0% | XGBoost (+10.2%) |

### 6.3 Temel Bulgular

1. **SMOTE en etkili teknik** - TÃ¼m modellerde Ã¶nemli iyileÅŸme saÄŸladÄ±
2. **Feature Engineering etkisiz** - Cleveland veri seti zaten iyi tasarlanmÄ±ÅŸ
3. **XGBoost en Ã§ok geliÅŸen model** - Baseline'da en kÃ¶tÃ¼, All Combined'da Ã§ok iyi
4. **Logistic Regression en tutarlÄ±** - Her senaryoda en iyi veya en iyi 2'de
5. **TÃ¼m teknikler birlikte +10% iyileÅŸme** saÄŸladÄ± (XGBoost iÃ§in)

---

## 7. Performans Metrikleri

### 7.1 Metrik TanÄ±mlarÄ±

| Metrik | FormÃ¼l | TÄ±bbi Yorumu |
|--------|--------|--------------|
| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Genel doÄŸruluk |
| **Recall** | TP/(TP+FN) | GerÃ§ek hastalarÄ±n kaÃ§Ä±nÄ± yakaladÄ±k |
| **F1-Score** | 2Ã—(PÃ—R)/(P+R) | Precision ve Recall dengesi |
| **AUC-ROC** | Area Under ROC | SÄ±nÄ±f ayÄ±rt etme yeteneÄŸi |

### 7.2 TÄ±bbi BaÄŸlamda Metrik Ã–nceliÄŸi

**Kritik:** TÄ±bbi taramada **Recall** en Ã¶nemli metriktir.

- **False Negative (Tip II Hata):** Hastaya "saÄŸlÄ±klÄ±" demek â†’ Tedavi gecikmesi
- **False Positive (Tip I Hata):** SaÄŸlÄ±klÄ±ya "hasta" demek â†’ Gereksiz testler

**SonuÃ§:** FN maliyeti >> FP maliyeti

---

## 8. Model SeÃ§imi Ã–nerileri

### 8.1 Uygulama SenaryolarÄ±na GÃ¶re

| Senaryo | Ã–nerilen Model | GerekÃ§e |
|---------|----------------|---------|
| **Tarama ProgramÄ±** | LR + All Combined | En yÃ¼ksek Recall (0.824) |
| **Klinik Karar Destek** | Logistic Regression | Yorumlanabilir, tutarlÄ± |
| **SÄ±nÄ±rlÄ± Kaynak** | LR + SMOTE | Ä°yi performans, hÄ±zlÄ± |

### 8.2 Nihai Ã–neri

**Logistic Regression + All Combined Pipeline:**
- F1-Score: 0.843Â±0.064
- AUC: 0.916Â±0.048
- Recall: 0.824 (hastalarÄ±n %82.4'Ã¼nÃ¼ yakalama)
- Avantaj: Yorumlanabilir, hÄ±zlÄ±, dÃ¼ÅŸÃ¼k varyans

---

## 9. Teknik Uygulama

### 9.1 KullanÄ±lan KÃ¼tÃ¼phaneler

```python
# Veri Ä°ÅŸleme
pandas>=2.0, numpy>=1.24

# Makine Ã–ÄŸrenmesi
scikit-learn>=1.3, xgboost>=2.0, imbalanced-learn>=0.11

# Optimizasyon
optuna>=3.4

# GÃ¶rselleÅŸtirme
matplotlib>=3.7, seaborn>=0.12
```

### 9.2 Kod DosyalarÄ±

| Dosya | AÃ§Ä±klama |
|-------|----------|
| `main_5_revised_scenarios.py` | 6 Senaryo karÅŸÄ±laÅŸtÄ±rmasÄ± |
| `scenario_results_*/` | SonuÃ§ klasÃ¶rleri (tarih damgalÄ±) |

### 9.3 Ã‡alÄ±ÅŸtÄ±rma

```bash
python main_5_revised_scenarios.py
```

---

## 10. SonuÃ§

Bu Ã§alÄ±ÅŸmada UCI Heart Disease Cleveland veri seti Ã¼zerinde 6 senaryo ile kapsamlÄ± bir ablation study gerÃ§ekleÅŸtirilmiÅŸtir.

### Temel Bulgular:

1. âœ… **SMOTE en etkili teknik** (+3.8% ortalama F1)
2. âœ… **Logistic Regression en tutarlÄ± model** (her senaryoda top-2)
3. âœ… **XGBoost en Ã§ok geliÅŸen model** (+10.2% All Combined'da)
4. âŒ **Feature Engineering etkisiz** (Cleveland zaten iyi tasarlanmÄ±ÅŸ)
5. âœ… **TÃ¼m teknikler birlikte** F1=0.843 elde edildi

### TÄ±bbi Ã–neri:

Tarama programlarÄ±nda **Logistic Regression + SMOTE + Optuna** kombinasyonu Ã¶nerilir:
- YÃ¼ksek Recall (%82+) ile hasta yakalama
- Yorumlanabilir model (klinik aÃ§Ä±klama)
- DÃ¼ÅŸÃ¼k hesaplama maliyeti

---

## Kaynaklar

1. UCI Machine Learning Repository - Heart Disease Dataset
2. Akiba, T., et al. (2019). Optuna: Hyperparameter Optimization Framework
3. Chawla, N. V., et al. (2002). SMOTE: Synthetic Minority Over-sampling

---

**Rapor Sonu**

_Son GÃ¼ncelleme: 1 Ocak 2025_
