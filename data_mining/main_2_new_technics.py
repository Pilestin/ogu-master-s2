"""
UCI Heart Disease Veri Seti - Ä°leri Seviye Analiz ve Modelleme
==============================================================

Bu script aim.md dosyasÄ±ndaki Ã¶nerileri uygular:
1. KNN Imputer ile eksik veri doldurma
2. AykÄ±rÄ± deÄŸer baskÄ±lama (Winsorizing)
3. Ã–zellik mÃ¼hendisliÄŸi
4. SMOTE ile sÄ±nÄ±f dengeleme
5. Stratified 10-Fold CV ile model karÅŸÄ±laÅŸtÄ±rma
6. SHAP analizi ve gÃ¶rselleÅŸtirmeler

NOT: Sadece Cleveland veri seti kullanÄ±lmaktadÄ±r.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import os

# Preprocessing
from sklearn.impute import KNNImputer
from sklearn.preprocessing import RobustScaler, LabelEncoder
from scipy.stats import mstats

# Modeller
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier

# Validasyon ve Metrikler
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    roc_curve, auc, f1_score, recall_score, 
    precision_score, accuracy_score, roc_auc_score
)

# Dengesiz Veri
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# SHAP
import shap

import warnings
warnings.filterwarnings('ignore')

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.style.use('seaborn-v0_8-whitegrid')

# Global deÄŸiÅŸken: SonuÃ§ klasÃ¶rÃ¼
RESULTS_DIR = None

def create_results_folder():
    """Tarih ve saat damgalÄ± sonuÃ§ klasÃ¶rÃ¼ oluÅŸtur"""
    global RESULTS_DIR
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    RESULTS_DIR = Path(__file__).parent / f"results_{timestamp}"
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    print(f"\nğŸ“‚ SonuÃ§ klasÃ¶rÃ¼ oluÅŸturuldu: {RESULTS_DIR}")
    return RESULTS_DIR
sns.set_palette("husl")

# ============================================================
# 1. VERÄ° YÃœKLEME VE Ä°NCELEME
# ============================================================

def load_and_explore_data(filepath):
    """Veriyi yÃ¼kle ve temel istatistikleri gÃ¶ster"""
    print("=" * 60)
    print("1. VERÄ° YÃœKLEME VE Ä°NCELEME")
    print("=" * 60)
    
    df = pd.read_csv(filepath)
    
    print(f"\nğŸ“Š Veri Seti Boyutu: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
    print(f"\nğŸ“‹ SÃ¼tunlar: {list(df.columns)}")
    
    # Sadece Cleveland verilerini filtrele
    print("\nğŸ” Sadece Cleveland verileri filtreleniyor...")
    df = df[df['dataset'] == 'Cleveland'].copy()
    print(f"   âœ“ Cleveland veri sayÄ±sÄ±: {len(df)} satÄ±r")
    
    # Hedef deÄŸiÅŸken daÄŸÄ±lÄ±mÄ±
    print("\nğŸ¯ Hedef DeÄŸiÅŸken (num) DaÄŸÄ±lÄ±mÄ±:")
    print(df['num'].value_counts())
    
    # Binary sÄ±nÄ±flandÄ±rma iÃ§in: 0 = saÄŸlÄ±klÄ±, 1+ = hasta
    df['target'] = (df['num'] > 0).astype(int)
    
    print("\nğŸ¯ Binary Hedef (0=SaÄŸlÄ±klÄ±, 1=Hasta) DaÄŸÄ±lÄ±mÄ±:")
    print(df['target'].value_counts())
    print(f"Hasta oranÄ±: {df['target'].mean():.2%}")
    
    # Eksik deÄŸerler
    print("\nâ“ Eksik DeÄŸerler:")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100
    missing_df = pd.DataFrame({'Eksik': missing, '%': missing_pct})
    print(missing_df[missing_df['Eksik'] > 0])
    
    return df

# ============================================================
# 2. VERÄ° Ã–NÄ°ÅLEME
# ============================================================

def preprocess_data(df):
    """Veri Ã¶niÅŸleme adÄ±mlarÄ±"""
    print("\n" + "=" * 60)
    print("2. VERÄ° Ã–NÄ°ÅLEME")
    print("=" * 60)
    
    df_processed = df.copy()
    
    # ----------------------------------------------------------
    # A. Kategorik deÄŸiÅŸkenleri sayÄ±sallaÅŸtÄ±r
    # ----------------------------------------------------------
    print("\nğŸ“ Kategorik deÄŸiÅŸkenler encode ediliyor...")
    
    # Kategorik sÃ¼tunlarÄ± belirle
    categorical_cols = ['sex', 'dataset', 'cp', 'restecg', 'exang', 'slope', 'thal', 'fbs']
    
    label_encoders = {}
    for col in categorical_cols:
        if col in df_processed.columns:
            le = LabelEncoder()
            # NaN deÄŸerleri geÃ§ici olarak 'missing' ile deÄŸiÅŸtir
            df_processed[col] = df_processed[col].fillna('missing')
            df_processed[col] = le.fit_transform(df_processed[col].astype(str))
            label_encoders[col] = le
            print(f"  âœ“ {col}: {len(le.classes_)} kategori")
    
    # ----------------------------------------------------------
    # B. SayÄ±sal sÃ¼tunlarÄ± belirle
    # ----------------------------------------------------------
    # id, num ve target sÃ¼tunlarÄ±nÄ± hariÃ§ tut
    exclude_cols = ['id', 'num', 'target']
    numeric_cols = [col for col in df_processed.select_dtypes(include=[np.number]).columns 
                   if col not in exclude_cols]
    
    print(f"\nğŸ“Š SayÄ±sal sÃ¼tunlar ({len(numeric_cols)}): {numeric_cols}")
    
    # ----------------------------------------------------------
    # C. KNN Imputer ile eksik deÄŸer doldurma
    # ----------------------------------------------------------
    print("\nğŸ”§ KNN Imputer uygulanÄ±yor (n_neighbors=5)...")
    
    # Eksik deÄŸer olan sÃ¼tunlarÄ± bul
    missing_before = df_processed[numeric_cols].isnull().sum().sum()
    
    imputer = KNNImputer(n_neighbors=5, weights='uniform')
    df_processed[numeric_cols] = imputer.fit_transform(df_processed[numeric_cols])
    
    missing_after = df_processed[numeric_cols].isnull().sum().sum()
    print(f"  âœ“ Eksik deÄŸerler: {missing_before} â†’ {missing_after}")
    
    # ----------------------------------------------------------
    # D. AykÄ±rÄ± deÄŸer baskÄ±lama (Winsorizing)
    # ----------------------------------------------------------
    print("\nğŸ“ˆ AykÄ±rÄ± deÄŸer baskÄ±lama (Winsorizing %5-%95)...")
    
    # Sadece sÃ¼rekli sayÄ±sal deÄŸiÅŸkenlere uygula
    continuous_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']
    
    for col in continuous_cols:
        if col in df_processed.columns:
            before_min, before_max = df_processed[col].min(), df_processed[col].max()
            df_processed[col] = mstats.winsorize(df_processed[col], limits=[0.05, 0.05])
            after_min, after_max = df_processed[col].min(), df_processed[col].max()
            print(f"  âœ“ {col}: [{before_min:.1f}, {before_max:.1f}] â†’ [{after_min:.1f}, {after_max:.1f}]")
    
    # ----------------------------------------------------------
    # E. Ã–zellik MÃ¼hendisliÄŸi
    # ----------------------------------------------------------
    print("\nğŸ”¬ Ã–zellik MÃ¼hendisliÄŸi...")
    
    # 1. Risk Skoru: YaÅŸ Ã— Kolesterol (normalize)
    df_processed['risk_score'] = (df_processed['age'] * df_processed['chol']) / 10000
    print("  âœ“ risk_score = age Ã— chol / 10000")
    
    # 2. YaÅŸ Kategorileri (Binning)
    df_processed['age_group'] = pd.cut(
        df_processed['age'], 
        bins=[0, 40, 55, 70, 100],
        labels=[0, 1, 2, 3]  # 0=GenÃ§, 1=Orta, 2=Risk, 3=YÃ¼ksek Risk
    ).astype(int)
    print("  âœ“ age_group: 0=GenÃ§(<40), 1=Orta(40-55), 2=Risk(55-70), 3=YÃ¼ksek(70+)")
    
    # 3. Kalp HÄ±zÄ± / YaÅŸ OranÄ±
    df_processed['hr_age_ratio'] = df_processed['thalch'] / df_processed['age']
    print("  âœ“ hr_age_ratio = thalch / age")
    
    # 4. Kan BasÄ±ncÄ± Ã— Kolesterol etkileÅŸimi
    df_processed['bp_chol_interaction'] = (df_processed['trestbps'] * df_processed['chol']) / 10000
    print("  âœ“ bp_chol_interaction = trestbps Ã— chol / 10000")
    
    return df_processed, label_encoders

# ============================================================
# 3. Ã–ZELLÄ°K VE HEDEF DEÄÄ°ÅKEN AYIRMA
# ============================================================

def prepare_features_target(df):
    """Ã–zellik ve hedef deÄŸiÅŸkenleri ayÄ±r"""
    print("\n" + "=" * 60)
    print("3. Ã–ZELLÄ°K VE HEDEF DEÄÄ°ÅKEN AYIRMA")
    print("=" * 60)
    
    # HariÃ§ tutulacak sÃ¼tunlar
    exclude_cols = ['id', 'num', 'target', 'dataset']
    
    # Ã–zellik sÃ¼tunlarÄ±
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    X = df[feature_cols].values
    y = df['target'].values
    
    print(f"\nğŸ“Š Ã–zellik boyutu: {X.shape}")
    print(f"ğŸ¯ Hedef daÄŸÄ±lÄ±mÄ±: SaÄŸlÄ±klÄ±={sum(y==0)}, Hasta={sum(y==1)}")
    print(f"ğŸ“‹ KullanÄ±lan Ã¶zellikler ({len(feature_cols)}):")
    for i, col in enumerate(feature_cols, 1):
        print(f"   {i:2d}. {col}")
    
    return X, y, feature_cols

# ============================================================
# 4. MODEL TANIMLAMA
# ============================================================

def get_models():
    """Model sÃ¶zlÃ¼ÄŸÃ¼ oluÅŸtur"""
    models = {
        'Logistic Regression': LogisticRegression(
            max_iter=1000, 
            random_state=42,
            class_weight='balanced'
        ),
        'Random Forest': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        ),
        'SVM (RBF)': SVC(
            kernel='rbf',
            probability=True,
            random_state=42,
            class_weight='balanced'
        ),
        'XGBoost': XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
    }
    return models

# ============================================================
# 5. CROSS-VALIDATION Ä°LE MODEL DEÄERLENDÄ°RME
# ============================================================

def evaluate_models(X, y, feature_names):
    """Stratified 10-Fold CV ile modelleri deÄŸerlendir"""
    print("\n" + "=" * 60)
    print("4. MODEL DEÄERLENDÄ°RME (Stratified 10-Fold CV)")
    print("=" * 60)
    
    # RobustScaler kullan
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X)
    
    # SMOTE uygula
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)
    print(f"\nâš–ï¸ SMOTE sonrasÄ±: SaÄŸlÄ±klÄ±={sum(y_resampled==0)}, Hasta={sum(y_resampled==1)}")
    
    # SonuÃ§ tablosu
    results = []
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    models = get_models()
    
    print("\n" + "-" * 80)
    print(f"{'Model':<25} {'Accuracy':<15} {'Precision':<15} {'Recall':<15} {'F1-Score':<15} {'AUC':<10}")
    print("-" * 80)
    
    best_model = None
    best_f1 = 0
    
    for name, model in models.items():
        # Cross-validation metrikleri
        acc_scores = cross_val_score(model, X_resampled, y_resampled, cv=cv, scoring='accuracy')
        prec_scores = cross_val_score(model, X_resampled, y_resampled, cv=cv, scoring='precision')
        rec_scores = cross_val_score(model, X_resampled, y_resampled, cv=cv, scoring='recall')
        f1_scores = cross_val_score(model, X_resampled, y_resampled, cv=cv, scoring='f1')
        auc_scores = cross_val_score(model, X_resampled, y_resampled, cv=cv, scoring='roc_auc')
        
        # SonuÃ§larÄ± yazdÄ±r
        print(f"{name:<25} "
              f"{acc_scores.mean():.3f}Â±{acc_scores.std():.3f}  "
              f"{prec_scores.mean():.3f}Â±{prec_scores.std():.3f}  "
              f"{rec_scores.mean():.3f}Â±{rec_scores.std():.3f}  "
              f"{f1_scores.mean():.3f}Â±{f1_scores.std():.3f}  "
              f"{auc_scores.mean():.3f}")
        
        results.append({
            'Model': name,
            'Accuracy': f"{acc_scores.mean():.3f}Â±{acc_scores.std():.3f}",
            'Precision': f"{prec_scores.mean():.3f}Â±{prec_scores.std():.3f}",
            'Recall': f"{rec_scores.mean():.3f}Â±{rec_scores.std():.3f}",
            'F1-Score': f"{f1_scores.mean():.3f}Â±{f1_scores.std():.3f}",
            'AUC': f"{auc_scores.mean():.3f}Â±{auc_scores.std():.3f}",
            'F1_mean': f1_scores.mean()
        })
        
        # En iyi modeli seÃ§ (F1-Score'a gÃ¶re)
        if f1_scores.mean() > best_f1:
            best_f1 = f1_scores.mean()
            best_model = (name, model)
    
    print("-" * 80)
    print(f"\nğŸ† En Ä°yi Model (F1-Score): {best_model[0]} ({best_f1:.3f})")
    
    return results, X_scaled, X_resampled, y_resampled, best_model, scaler

# ============================================================
# 6. GÃ–RSELLEÅTÄ°RMELER
# ============================================================

def plot_confusion_matrices(X, y, models):
    """TÃ¼m modeller iÃ§in confusion matrix"""
    print("\n" + "=" * 60)
    print("5. CONFUSION MATRIX GÃ–RSELLEÅTÄ°RME")
    print("=" * 60)
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()
    
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    for idx, (name, model) in enumerate(models.items()):
        y_pred = cross_val_predict(model, X, y, cv=cv)
        cm = confusion_matrix(y, y_pred)
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                   xticklabels=['SaÄŸlÄ±klÄ±', 'Hasta'],
                   yticklabels=['SaÄŸlÄ±klÄ±', 'Hasta'])
        axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')
        axes[idx].set_xlabel('Tahmin')
        axes[idx].set_ylabel('GerÃ§ek')
        
        # Tip I ve Tip II hatalarÄ±
        tn, fp, fn, tp = cm.ravel()
        axes[idx].text(0.5, -0.15, 
                       f'FP (Tip I): {fp} | FN (Tip II): {fn}',
                       transform=axes[idx].transAxes,
                       ha='center', fontsize=9, color='red')
    
    plt.suptitle('Confusion Matrix KarÅŸÄ±laÅŸtÄ±rmasÄ±\n(Stratified 10-Fold CV)', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    save_path = RESULTS_DIR / 'confusion_matrices.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"âœ“ {save_path.name} kaydedildi")

def plot_roc_curves(X, y, models):
    """ROC eÄŸrileri"""
    print("\n" + "=" * 60)
    print("6. ROC EÄRÄ°LERÄ°")
    print("=" * 60)
    
    plt.figure(figsize=(10, 8))
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    for idx, (name, model) in enumerate(models.items()):
        y_proba = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]
        fpr, tpr, _ = roc_curve(y, y_proba)
        roc_auc = auc(fpr, tpr)
        
        plt.plot(fpr, tpr, color=colors[idx], linewidth=2,
                label=f'{name} (AUC = {roc_auc:.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.500)')
    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)
    plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)
    plt.title('ROC EÄŸrileri KarÅŸÄ±laÅŸtÄ±rmasÄ±\n(Stratified 10-Fold CV)', 
             fontsize=14, fontweight='bold')
    plt.legend(loc='lower right', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    save_path = RESULTS_DIR / 'roc_curves.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"âœ“ {save_path.name} kaydedildi")

def plot_feature_importance(X, y, feature_names):
    """Feature importance (Random Forest)"""
    print("\n" + "=" * 60)
    print("7. Ã–ZELLÄ°K Ã–NEMÄ° (Random Forest)")
    print("=" * 60)
    
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X, y)
    
    importance = rf.feature_importances_
    indices = np.argsort(importance)[::-1]
    
    plt.figure(figsize=(12, 8))
    colors = plt.cm.viridis(np.linspace(0, 0.8, len(feature_names)))
    
    plt.barh(range(len(feature_names)), importance[indices][::-1], 
            color=colors, edgecolor='black', linewidth=0.5)
    plt.yticks(range(len(feature_names)), [feature_names[i] for i in indices][::-1])
    plt.xlabel('Ã–nem Skoru', fontsize=12)
    plt.title('Ã–zellik Ã–nem SÄ±ralamasÄ± (Random Forest)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    save_path = RESULTS_DIR / 'feature_importance.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"âœ“ {save_path.name} kaydedildi")
    
    print("\nğŸ“Š Ã–zellik Ã–nem SÄ±ralamasÄ±:")
    for i, idx in enumerate(indices[:10], 1):
        print(f"   {i:2d}. {feature_names[idx]:<25} {importance[idx]:.4f}")

# ============================================================
# 7. SHAP ANALÄ°ZÄ°
# ============================================================

def shap_analysis(X, y, feature_names):
    """SHAP deÄŸerleri ile model aÃ§Ä±klanabilirliÄŸi"""
    print("\n" + "=" * 60)
    print("8. SHAP ANALÄ°ZÄ° (Model AÃ§Ä±klanabilirliÄŸi)")
    print("=" * 60)
    
    # X'i DataFrame'e Ã§evir
    X_df = pd.DataFrame(X, columns=feature_names)
    
    # Random Forest modeli eÄŸit
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_df, y)
    
    # SHAP deÄŸerlerini hesapla
    print("\nâ³ SHAP deÄŸerleri hesaplanÄ±yor (bu biraz zaman alabilir)...")
    
    # Daha kÃ¼Ã§Ã¼k bir Ã¶rneklem al (hÄ±z iÃ§in)
    sample_size = min(200, len(X_df))
    X_sample = X_df.sample(n=sample_size, random_state=42)
    
    # TreeExplainer kullan
    explainer = shap.TreeExplainer(rf)
    shap_values = explainer.shap_values(X_sample)
    
    # Ã–zet grafiÄŸi
    plt.figure(figsize=(12, 8))
    # shap_values[1] hasta sÄ±nÄ±fÄ± iÃ§in
    if isinstance(shap_values, list):
        shap_vals = shap_values[1]
    else:
        shap_vals = shap_values
    
    shap.summary_plot(shap_vals, X_sample, show=False)
    plt.title('SHAP Ã–zet GrafiÄŸi (Hasta SÄ±nÄ±fÄ±)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    save_path = RESULTS_DIR / 'shap_summary.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"âœ“ {save_path.name} kaydedildi")
    
    # Bar grafiÄŸi
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_vals, X_sample, plot_type='bar', show=False)
    plt.title('SHAP Ã–zellik Ã–nem SÄ±ralamasÄ±', fontsize=14, fontweight='bold')
    plt.tight_layout()
    save_path = RESULTS_DIR / 'shap_importance.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"âœ“ {save_path.name} kaydedildi")
    
    return shap_values, explainer

# ============================================================
# 8. SONUÃ‡ RAPORU
# ============================================================

def generate_report(results):
    """SonuÃ§ raporu oluÅŸtur"""
    print("\n" + "=" * 60)
    print("9. SONUÃ‡ RAPORU")
    print("=" * 60)
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     UCI HEART DISEASE ANALÄ°ZÄ°                    â•‘
â•‘                       SONUÃ‡ RAPORU                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ UYGULANAN TEKNÄ°KLER:
   1. KNN Imputer ile eksik deÄŸer doldurma (k=5)
   2. AykÄ±rÄ± deÄŸer baskÄ±lama (Winsorizing %5-%95)
   3. Ã–zellik MÃ¼hendisliÄŸi:
      - Risk skoru (yaÅŸ Ã— kolesterol)
      - YaÅŸ kategorileri (binning)
      - Kalp hÄ±zÄ±/yaÅŸ oranÄ±
      - Kan basÄ±ncÄ± Ã— kolesterol etkileÅŸimi
   4. RobustScaler ile Ã¶lÃ§ekleme
   5. SMOTE ile sÄ±nÄ±f dengeleme
   6. Stratified 10-Fold Cross Validation

ğŸ“Š MODEL KARÅILAÅTIRMASI:
""")
    
    # SonuÃ§ tablosu
    df_results = pd.DataFrame(results)
    print(df_results[['Model', 'Accuracy', 'Recall', 'F1-Score', 'AUC']].to_string(index=False))
    
    print("""
ğŸ’¡ Ã–NEMLÄ° NOTLAR:
   - Recall (DuyarlÄ±lÄ±k) tÄ±bbi Ã§alÄ±ÅŸmalarda kritiktir
   - False Negative (Tip II hata) minimizasyonu Ã¶nemli
   - SHAP deÄŸerleri model kararlarÄ±nÄ± aÃ§Ä±klar
   
ğŸ“ˆ Ã–NERÄ°LER:
   - YÃ¼ksek Recall'a sahip modeli tercih edin
   - Confusion matrix'teki FN sayÄ±sÄ±nÄ± minimize edin
   - SHAP grafiklerini raporda kullanÄ±n
""")
    
    # CSV olarak kaydet
    save_path = RESULTS_DIR / 'model_results.csv'
    df_results.to_csv(save_path, index=False)
    print(f"\nâœ“ {save_path.name} kaydedildi")

# ============================================================
# ANA FONKSÄ°YON
# ============================================================

def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    print("\n" + "=" * 60)
    print("  UCI HEART DISEASE - Ä°LERÄ° SEVÄ°YE ANALÄ°Z PAKETÄ°")
    print("  (Sadece Cleveland Veri Seti)")
    print("=" * 60)
    
    # SonuÃ§ klasÃ¶rÃ¼ oluÅŸtur
    create_results_folder()
    
    # Veri yolu
    data_path = Path(__file__).parent / 'data' / 'heart_disease_uci.csv'
    
    # 1. Veri yÃ¼kleme
    df = load_and_explore_data(data_path)
    
    # 2. Veri Ã¶niÅŸleme
    df_processed, label_encoders = preprocess_data(df)
    
    # 3. Ã–zellik ve hedef ayÄ±rma
    X, y, feature_names = prepare_features_target(df_processed)
    
    # 4. Model deÄŸerlendirme
    results, X_scaled, X_resampled, y_resampled, best_model, scaler = evaluate_models(X, y, feature_names)
    
    # 5. Confusion Matrix
    models = get_models()
    plot_confusion_matrices(X_resampled, y_resampled, models)
    
    # 6. ROC EÄŸrileri
    plot_roc_curves(X_resampled, y_resampled, models)
    
    # 7. Ã–zellik Ã¶nemi
    plot_feature_importance(X_resampled, y_resampled, feature_names)
    
    # 8. SHAP Analizi
    shap_values, explainer = shap_analysis(X_resampled, y_resampled, feature_names)
    
    # 9. SonuÃ§ raporu
    generate_report(results)
    
    print("\n" + "=" * 60)
    print("  ANALÄ°Z TAMAMLANDI!")
    print(f"  SonuÃ§lar: {RESULTS_DIR}")
    print("  OluÅŸturulan dosyalar:")
    print("    - confusion_matrices.png")
    print("    - roc_curves.png")
    print("    - feature_importance.png")
    print("    - shap_summary.png")
    print("    - shap_importance.png")
    print("    - model_results.csv")
    print("=" * 60)

if __name__ == "__main__":
    main()
